{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff9ddb8",
   "metadata": {},
   "source": [
    "# Stain normalization notebook\n",
    "This notebook demonstrates the use of deep learning models to train content encoder and attribute encoder for the stain normalization.\n",
    "Changes compared to the normal code\n",
    "<ol>\n",
    "    <li>Add LeakyReLU to enforce non-negative constraints to the content encoder.</li>\n",
    "</ol>\n",
    "\n",
    "Reference\n",
    "1. [Yibo's color normalization exp](https://docs.google.com/presentation/d/1MttGX3S6nWdlUOzHGhyHuAIWgttYA2EzEsehvELFBks/edit#slide=id.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment to run the workflow\n",
    "import os\n",
    "import sys\n",
    "### setting up env variables before running the dynamic workflows\n",
    "os.environ[\"MLENV_NAME\"] = \"tan.nguyen/overlay_test\" # your_mle_username/your_mle_envname\n",
    "os.environ[\"MLPLATFORM_BRANCH\"] = \"pdac_fibrosis\"\n",
    "os.environ[\"JUPYTERHUB_USER\"] = \"tan.nguyen\"\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e304fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from typing import Optional\n",
    "from tempfile import TemporaryDirectory\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from IPython.core.debugger import set_trace\n",
    "import lpips\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from pathai.dynamic import set_jabba\n",
    "from pathai.dynamic import set_local\n",
    "from pathai.dynamic import Jabba\n",
    "from pathai.dynamic import jmap\n",
    "from pathai.dynamic import AssetRef\n",
    "from pathai.io.samples_flexible_io import SamplesWriter\n",
    "from pathai.io.samples_flexible_io import SamplesReader\n",
    "import pathai.api.slides.slides as slide_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IMAGE_SIZE_PIXELS = 512\n",
    "NUM_STAIN_VECTORS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2315c460",
   "metadata": {},
   "source": [
    "## 1. Prepare the training slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathai.api.monocle.slides import get_slide_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5dfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Samples:\n",
    "    core_name: str\n",
    "    row_idx: int\n",
    "    col_idx: int\n",
    "    slide_id: Optional[int] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathai.dynamic import SlideReference\n",
    "from pathai.dynamic import task\n",
    "\n",
    "def _he_tissue_mask(slide_id: int, downsampling_factor) -> np.ndarray:\n",
    "    # Compute the tissue mask from the input slide id.\n",
    "    CLOSING_KEREL_SIZE_PIXELS = 6\n",
    "    print(f\"Computing the tissue mask for slide {slide_id}\")\n",
    "    slide_reference = SlideReference(int(slide_id))\n",
    "    with slide_reference.read_object() as slide:\n",
    "        original_mpp = slide.mpp\n",
    "        im = slide.view_at_mpp(slide.mpp * downsampling_factor, mpp_tolerance=0.1)\n",
    "        # Due to histogram adjustment, the background value may not be the same, need to use Otsu for this.\n",
    "        im = cv2.cvtColor(im[:, :, :], cv2.COLOR_BGR2GRAY )\n",
    "        laplacian = cv2.Laplacian(im,cv2.CV_32F)\n",
    "        sobelx = cv2.Sobel(im,cv2.CV_32F,1,0,ksize=5)\n",
    "        sobely = cv2.Sobel(im,cv2.CV_32F,0,1,ksize=5)\n",
    "        grad_sqr = np.sqrt(sobelx ** 2 + sobely**2).astype(np.uint8)\n",
    "        threshold_val, tissue_mask = cv2.threshold(grad_sqr, 0, 255, cv2.THRESH_OTSU)\n",
    "        kernel = np.ones((CLOSING_KEREL_SIZE_PIXELS, CLOSING_KEREL_SIZE_PIXELS), np.uint8)\n",
    "        return cv2.morphologyEx(tissue_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "def compute_usable_tissue_fraction(\n",
    "    center_coords: Tuple[int, int], patch_size_pixels: int, tissue_mask: np.ndarray\n",
    ") -> float:\n",
    "    row_idx, col_idx = center_coords\n",
    "    nrows, ncols = tissue_mask.shape\n",
    "    half_patch_size_pixels = patch_size_pixels // 2\n",
    "    if not half_patch_size_pixels <= row_idx <= nrows - half_patch_size_pixels:\n",
    "        raise ValueError(\n",
    "            f\"row_idx {row_idx} must be between [{half_patch_size_pixels}, {nrows - half_patch_size_pixels}]\"\n",
    "        )\n",
    "    if not half_patch_size_pixels <= col_idx <= ncols - half_patch_size_pixels:\n",
    "        raise ValueError(\n",
    "            f\"row_idx {col_idx} must be between [{half_patch_size_pixels}, {ncols - half_patch_size_pixels}]\"\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        float(\n",
    "            np.sum(\n",
    "                tissue_mask[\n",
    "                    row_idx - half_patch_size_pixels : row_idx + half_patch_size_pixels,\n",
    "                    col_idx - half_patch_size_pixels : col_idx + half_patch_size_pixels,\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        / float(patch_size_pixels ** 2)\n",
    "        / 255\n",
    "    )\n",
    "\n",
    "\n",
    "@task(node_family=\"high-mem\", slots=4)\n",
    "def generate_wsi_sampling_points(\n",
    "    slide_id: int,\n",
    "    num_candidates_per_core: int = 100000, \n",
    "    patch_size_pixels: int = INPUT_IMAGE_SIZE_PIXELS, \n",
    "    minimum_tissue_fraction_to_be_selected: float = 0.8,\n",
    ") -> List[Samples]:\n",
    "    \"\"\"Generates a list of Sample for each slide.\n",
    "    \n",
    "    Args:\n",
    "        slide_id: The name of the core to generate Samples.\n",
    "        num_candidates_per_core (optional): The number of candidates per core to consider for sampling coordinates. Defaults to 100000.\n",
    "        patch_size (optional): The size of each sampling patch. Defaults to INPUT_IMAGE_SIZE_PIXELS.\n",
    "        minimum_tissue_fraction_to_be_selected (optional): The number that specifies the minimum fraction of tissue for a candidate to be selected as a sampling\n",
    "            point. Defaults to be 0.1.\n",
    "            \n",
    "    Returns:\n",
    "        A list of Samples that contains the coordinates of the center point of each sampling patch.\n",
    "    \"\"\"\n",
    "    downsampling_factor = 8\n",
    "    print(\"Computing the tissue mask\")\n",
    "    tissue_mask = _he_tissue_mask(slide_id, downsampling_factor)\n",
    "    ds_nrows, ds_ncols = tissue_mask.shape\n",
    "    ds_patch_size_pixels = patch_size_pixels // downsampling_factor\n",
    "    ds_half_patch_size_pixels = ds_patch_size_pixels // 2\n",
    "    \n",
    "    grid_spacing = int(np.sqrt((ds_nrows - ds_patch_size_pixels) * (ds_ncols - ds_patch_size_pixels) / num_candidates_per_core))\n",
    "\n",
    "    print(\"Generating sampling candidate the tissue mask\")\n",
    "    sampling_candidates = product(range(ds_half_patch_size_pixels, ds_nrows - ds_half_patch_size_pixels, grid_spacing), \n",
    "                                  range(ds_half_patch_size_pixels, ds_ncols - ds_half_patch_size_pixels, grid_spacing))\n",
    "    \n",
    "    useful_tissue_fractions = map(lambda x : compute_usable_tissue_fraction(x, patch_size_pixels=ds_patch_size_pixels, tissue_mask=tissue_mask),\n",
    "                                  sampling_candidates)\n",
    "    \n",
    "    return[Samples(core_name=None, \n",
    "                   row_idx=int(x[0]) * downsampling_factor, \n",
    "                   col_idx=int(x[1]) * downsampling_factor,\n",
    "                   slide_id = slide_id,\n",
    "                  ) \n",
    "           for x, y in zip(sampling_candidates, useful_tissue_fractions) if y > minimum_tissue_fraction_to_be_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cdd9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_by_name: Dict[str, List[int]] ={\n",
    "    'daphne': ([568263, 568260, 568257, 568254, 568251, 568248, 568245, 568242, 568239, 568236, 568233, 568230, 568227, \n",
    "                568224, 568218, 568215, 568212, 568209, 568206, 568203, 568200, 568197, 568194, 568191, 568188, 568185, \n",
    "                568182, 568179, 568176, 568170, 568167, 567988, 568164, 568161, 568158, 568155, 568152, 568149, 568146, \n",
    "                568143, 568140, 568137, 568134, 568131, 568128, 568125, 568122, 568119, 568116, 568113, 568110, 568107, \n",
    "                568101, 568098, 568095, 568092, 568089, 568086, 568083, 568080, 568077, 568074, 568068, 568065, 568062, \n",
    "                568059, 568056, 568053, 568050, 568047, 568044, 568041, 568038, 568035, 568032, 568029, 568026, 568023, \n",
    "                568020], [568017, 568012, 568009, 568006, 568003, 568000, 567997, 567994, 567991]),\n",
    "    'p2_at2': ([336517, 336518, 338299, 338318, 338321, 338324, 338344, 340007, 340011, 340020, 340021, 340026, 340035, \n",
    "                340051, 340055, 340057, 340064, 340125, 340130, 340138, 340141, 340148, 340152, 340154, 340163, 342625,\n",
    "                342632, 342635, 342641, 342653, 342662, 342666, 342743, 342745, 342750, 342752, 342760, 342764, 342769, \n",
    "                342782, 342784, 342795, 342856, 342857, 342865, 342868, 342869, 342873, 342879, 342906, 342918, 342924,\n",
    "                342927, 342931, 342944, 342952, 342959, 343013, 343017, 343020, 343024, 343029, 343043, 343052, 343059,\n",
    "                343062, 343064, 343079, 343080, 343088, 343094, 343129, 344180, 344188, 344194, 344198, 344206, 344210,\n",
    "                344602, 344603, 344611, 344616, 344621, 344625, 344630, 344639, 344643, 344649, 344667, 344675, 344679, \n",
    "                344686, 344692, 344707, 344710, 344718, 344719, 344726, 344770, 344773, 344789, 344791, 344802, 344808,\n",
    "                344811, 344816, 344821, 344827, 344842, 344847, 344851, 344854, 344986, 345091, 345094, 345098, 345100,\n",
    "                345106, 345111, 345117, 345123, 345126, 345129, 345143, 345145, 345151, 345156, 345160, 345163, 345169,\n",
    "                345749, 345755, 345761, 345763, 345778, 345782, 345804, 345815, 345829, 345836, 345840, 345872, 345877,\n",
    "                345878, 345883, 345887, 345893, 345900, 345910, 345914, 345925, 345930, 345931, 345940, 346946, 346955,\n",
    "                346961, 346964, 346967, 346970, 346976, 346983, 346986, 346990, 346993, 347000, 347004, 347012, 347019,\n",
    "                347025, 347032, 347040, 347043, 347046, 347066, 347079, 347089, 347099, 347113, 347115, 347125, 347128,\n",
    "                347143, 347145, 347149, 347156, 347162, 347166, 347172, 347177, 347189, 347196, 347203, 347219, 347225,\n",
    "                347443, 347446, 347450, 347455, 349471, 349474, 349479, 349482, 349489, 349498, 349506, 349512, 349514,\n",
    "                349517, 349525, 349528, 349545, 349547, 349550, 349566, 349570, 349573, 349580, 349603, 349613, 349619,\n",
    "                349621, 349630, 349648, 349649, 349659, 349671, 349692, 349697, 349701, 349703, 349708, 349720, 349726,\n",
    "                351671, 351675, 465745, 465746, 465747, 465748, 465749, 465750, 465751, 465752, 465753, 465766, 465767,\n",
    "                465768, 465769, 465770, 465772, 465773, 465774, 465775, 465776, 465777, 465778, 465779, 465780, 465781,\n",
    "                465782, 465783, 465784, 465785, 465786, 465787, 465788, 465789, 465790, 465791, 465792, 465793, 465794,\n",
    "                465795, 465796, 465797, 465798, 465799, 465800, 465801, 465802, 465803, 465804, 465805, 465806, 465807,\n",
    "                465808, 465809, 465832, 465833, 465834, 465835, 465836, 465837],\n",
    "                [336527,338333,340069,340073,340076,340081,340086,340090,342676,342682,342685,342697,342700,342703,\n",
    "                342819,342832,342886,342889,342968,342971,342973,343098,343105,343107,344250,344256,344257,344262,\n",
    "                344264,344269,344282,344286,344289,344486,344494,344867,344869,344876,344898,344902,344913,344917,\n",
    "                344919,344928,344936,344940,344941,344950,345954,345976,345983,345988,346009,346012,346016,346020,\n",
    "                346026,346032,346037,346043,347227,347238,347242,347251,347254,347269,347274,347278,347286,347293,\n",
    "                347296,347307,347314,347317,351681,351684,351690,351700,465754,465755,465756,465757,465758,465759,\n",
    "                465760,465761,465762,465763,465764,465810,465811,465812,465813,465814,465815,465816,465817]),\n",
    "    'p2_gt450': ([330608,330609,330722,330725,330727,330734,330801,331136,331144,331162,331164,331170,331179,\n",
    "                331195,331199,331201,331208,331270,331275,331283,331286,331293,331297,331299,331308,336010,336017,\n",
    "                336020,336026,336038,336047,336051,336128,336130,336135,336145,336149,336154,336167,336169,336180,\n",
    "                336260,336262,336265,336276,336282,336284,336289,336292,336301,336304,336307,336313,336326,336334,\n",
    "                336341,336396,336400,336403,336407,336412,336426,336436,336446,336448,336463,336464,336472,336478,\n",
    "                336513,336634,336642,336648,336652,336660,336664,338433,338434,338442,338447,338452,338456,338462,\n",
    "                338471,338475,338481,338499,338507,338511,338518,338524,338550,338551,338558,340169,340172,340188,\n",
    "                340190,340201,340207,340210,340215,340220,340226,340241,340250,340253,340485,340488,340492,340494,\n",
    "                340500,340505,340511,340517,340520,340537,340545,340550,340554,340557,340563,340584,340595,340601,\n",
    "                340607,340609,340624,340628,340651,340662,340676,340683,340687,340693,340698,340699,340704,340708,\n",
    "                340714,340721,340731,340735,340746,340752,340753,340762,341007,341017,341023,341026,341029,341032,\n",
    "                341038,341045,341048,341052,341055,341062,341066,341074,341081,341087,341094,341102,341105,341108,\n",
    "                341129,341141,341151,341161,341175,341177,341187,341190,341205,341207,341211,341218,341224,341228,\n",
    "                341234,341239,341251,341258,341265,341281,341287,341504,341507,341511,341516,341520,341523,341528,\n",
    "                341531,341538,341548,341556,341562,341564,341567,341575,341578,341595,341597,341600,341616,341620,\n",
    "                341623,341630,341653,341664,341670,341682,341701,341702,341712,341724,341745,341750,341754,341756,\n",
    "                341761,341773,341794,341798,343708,343713,343719,343728,343729,343737,343741,343748,343750,343754,\n",
    "                343756,343760,343761,343765,343773,343774,343777,343778,343794,343805,343811,343819,343822,343825,\n",
    "                343831,343834,343838,343841,343846,343850,343855,343858,343862,343866,343871,343876,343889,343890,\n",
    "                343897,343901,343904,343911,343914,343919,344064,344067,344071,344073,344081,344084,344393,344397,\n",
    "                344399,344407,344409,344423,344425,344434], \n",
    "                [330743,331213,331217,331220,331225,331231,331235,336061,336067,336070,336082,336085,336088,336204,\n",
    "                336217,336227,336233,336350,336353,336355,336482,336489,336491,336685,336704,336710,336711,336716,\n",
    "                336718,336723,336736,336740,336743,336755,340266,340268,340275,340297,340302,340313,340317,340319,\n",
    "                340328,340336,340340,340341,340351,340776,340798,340805,340810,340831,340834,340838,340842,340848,\n",
    "                340854,340859,340865,341289,341300,341304,341313,341316,341331,341336,341340,341348,341355,341358,\n",
    "                341369,341376,341379,341804,341807,341813,341823,343926,343931,343933,343940,343971,343976,343979,\n",
    "                343985,344438,344442,344448,344450,344457,344461,344463,344467,344473,344474,344483]),\n",
    "    'p2_dp200': ([464595,464596,464597,464598,464599,464600,464601,464602,464603,464604,464605,464606,464607,464608,\n",
    "                464609,464610,464612,464613,464614,464615,464616,464617,464618,464619,464620,464621,464622,464623,\n",
    "                464624,464625,464626,464627,464628,464629,464630,464631,464632,464633,464634,464635,464636,464637,\n",
    "                464638,464639,464640,464641,464642,464643,464644,464645,465511,465512,465513,465514,465515,465516,\n",
    "                465517,465518,465530,465531,465532,465533,465534,465535,465536,465537,465538,465539,473544,473545,\n",
    "                473546,473547,473548,473549,473550,473569,473570,473571,473572,473573,473574,473575,473576,473577,\n",
    "                473578,473579,473580,473581,473582,473583,473585,473586,473587,473588,473589,473590,473591,474012,\n",
    "                474020,474021,474022,474023,474025,474026,474035,474036,474037,474038,474039,474040,474041,474042,\n",
    "                474043,474044,474045,474046,474047,474048,474049,474050,474051,474052,474053,474054,474067,474068,\n",
    "                474069,474070,474071,474072,474073,474074,474075,474076,474077,474078,474079,474080,474081,474082,\n",
    "                474083,474160,474162,474163,474178,474179,474180,474181,474182,474183,474184,474185,474186,474187,\n",
    "                474188,474189,474190,474191,474192,474193,474194,474422,474423,474424,474425,474426,474439,474440,\n",
    "                474441,474442,474443,474444,474445,474446,474447,474448,474450,479880,479884,479885,479886,479887,\n",
    "                479888,479889,479890,479895,479897,479898,479899,479900,479901,479903,479918,479919,479920,479923,\n",
    "                479924,479925,479926,479927,479929,479930,479931,479932,479936,479937,479938,479939,479940,479941,\n",
    "                479942,479943,479944,479945,479946,479947,479948,479949,479950,479951,479952,479959,479960,479961,\n",
    "                479962,479963,479964,479965,479966,479967,479968,479999,480000,480001,480002,480003,480004,480005,\n",
    "                480006,480007,480008,480009,480010,480011,480012,480013,480014,480015,480016,480017,480018,480019,\n",
    "                480020,480021,480022,480023,480024,480025,480026,480027,480028,480029,480892,480895,480896,480897,\n",
    "                480900,480901,480902,480903,480904,480905,480906,480907,480908,480909,480910,480911,480916,480928,\n",
    "                480929,480930,480931,480932,480933,480934,480935,480936,480948,484333],\n",
    "                [465519,465520,465521,465522,465523,465524,465525,465526,465527,465528,465529,465540,465541,465542,\n",
    "                465543,465544,465545,465546,470074,473551,473552,473553,473554,473555,473556,473557,473558,473559,\n",
    "                473560,473561,474013,474014,474015,474027,474028,474029,474164,474165,474166,474167,474168,474169,\n",
    "                474170,474171,474172,474173,474174,474175,474176,474427,474428,474429,474430,474431,474432,474433,\n",
    "                474434,474435,474436,479881,479882,479891,479892,479893,479894,479896,479904,479905,479906,479911,\n",
    "                479912,479933,479934,479953,479954,479955,479969,479970,479971,479972,479973,479974,479975,479976,\n",
    "                479977,479978,479979,479980,480899,480912,480913,480914,480915,480917,480918,480919,480922,480939]),\n",
    "    'p2_ufs': ([487962,487963,487964,487967,487968,487971,487972,487973,487974,487975,487976,487977,487979,487982,\n",
    "                487983,487985,487986,487987,487989,487991,487992,487993,487994,487995,487997,487999,488000,488001,\n",
    "                488002,488003,488004,488005,488006,488007,488009,488010,488012,488013,488015,488017,488018,488024,\n",
    "                488025,488026,488029,488030,488032,488033,488034,488035,488037,488038,488041,488045,488049,488050,\n",
    "                488052,488058,488060,488061,488065,488067,488069,488071,488072,488074,488075,488078,488079,488080,\n",
    "                488081,488083,488084,488090,488091,488092,488094,488095,488097,488099,488100,488101,488102,488103,\n",
    "                488106,488107,488108,488110,488111,488113,488116,488119,488120,488122,488123,488126,488128,488129,\n",
    "                488130,488132,488133,488135,488136,488137,488138,488139,488140,488144,488149,488150,488151,488154,\n",
    "                488155,488157,488159,488161,488162,488163,488164,488168,488171,488172,488173,488174,488175,488176,\n",
    "                488178,488179,488180,488181,488185,488186,488190,488191,488192,488194,488196,488197,488199,488200,\n",
    "                488201,488202,488206,488207,488208,488211,488213,488214,488215,488218,488222,488225,488227,488231,\n",
    "                488233,488235,488236,488237,488238,488241,488242,488244,488245,488246,488247,488250,488252,488255,\n",
    "                488256,488259,488260,488261,488263,488266,488267,488270,488271,488274,488275,488278,488279,488282,\n",
    "                488284,488288,488290,488291,488293,488294,488297,488298,488299,488300,488302,488304,488306,488307,\n",
    "                488309,488310,488311,488312,488313,488314,488315,488321,488323,488328,488329,488332,488333,488336,\n",
    "                488340,488344,488348,488349,488352,488353,488357,488358,488359,488361,488362,488365,488369,488371,\n",
    "                488375,488377,488379,488380,488382,488384,488385,488386,488392,488395,488397,488400,488402,488403,\n",
    "                488405,488409,488411,488412,488413,488415,488418,488420,488422,488425,488430,488432,488434,488437,\n",
    "                488438,488439,488441,488443,488445,488446,488448,488453,488454,488455,488457,488462,488464,488465,\n",
    "                488468,488469,488470,488471,488472,488473,488474,488475,488479,488480,488481,488482,488483,488484,\n",
    "                488489,488492,488493,488495,488496], \n",
    "                [487960,487961,487966,487969,487970,487984,487988,487990,487996,488008,488016,488019,488021,488023,\n",
    "                488027,488040,488042,488046,488048,488051,488054,488055,488057,488059,488063,488064,488068,488073,\n",
    "                488076,488089,488104,488105,488114,488115,488124,488127,488131,488143,488148,488156,488158,488166,\n",
    "                488170,488184,488187,488188,488189,488198,488204,488205,488209,488216,488219,488221,488232,488234,\n",
    "                488249,488254,488262,488268,488272,488277,488283,488285,488289,488316,488319,488322,488324,488325,\n",
    "                488326,488339,488347,488350,488351,488364,488374,488376,488388,488391,488394,488398,488416,488424,\n",
    "                488426,488428,488440,488442,488444,488449,488452,488463,488485,488486,488490,488491,488494]),\n",
    "    'tmas': ([530691, 237449], [])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b375d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_samples_per_domain = 100000\n",
    "train_domain_names = ['daphne', 'p2_gt450', 'p2_ufs']\n",
    "val_domain_names = ['p2_at2', 'p2_dp200']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e410f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(len(datasets_by_name[x][0]) for x in val_domain_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfa42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_jabba()\n",
    "train_lists: List[List[Samples]] = []\n",
    "for name in train_domain_names:\n",
    "    training_slide_ids, validation_slide_ids = datasets_by_name[name]\n",
    "    temp_samples_from_all_slides_of_one_domain = jmap(\n",
    "        generate_wsi_sampling_points, \n",
    "        training_slide_ids,\n",
    "        cache_key=f\"breast_cancer_generate_training_samples_for_{name}_dataset_80_pct_patch_size_512\").wait()\n",
    "    all_samples = np.random.permutation(\n",
    "            list(itertools.chain.from_iterable(temp_samples_from_all_slides_of_one_domain))\n",
    "        )[:max_num_samples_per_domain]\n",
    "    train_lists.append(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b12ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_jabba()\n",
    "val_lists: List[List[Samples]] = []\n",
    "for name in val_domain_names:\n",
    "    training_slide_ids, validation_slide_ids = datasets_by_name[name]\n",
    "    task_notes = f\"breast_cancer_generate_training_samples_for_{name}_dataset_80_pct_patch_size_512\"\n",
    "    print(task_notes)\n",
    "    temp_samples_from_all_slides_of_one_domain = jmap(\n",
    "        generate_wsi_sampling_points, \n",
    "        training_slide_ids,\n",
    "        cache_key=task_notes).wait()\n",
    "    all_samples = np.random.permutation(\n",
    "            list(itertools.chain.from_iterable(temp_samples_from_all_slides_of_one_domain))\n",
    "        )[:max_num_samples_per_domain]\n",
    "    val_lists.append(all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753de2e",
   "metadata": {},
   "source": [
    "## 2. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c54ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from skimage.transform import rotate\n",
    "from typing import Dict\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797bc7a",
   "metadata": {},
   "source": [
    "Define transformations including RGB to transmittance and transmittance to absorbance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFlip(object):\n",
    "    \"\"\"Flips the images up/down, left/right randomly with probability of 0.5\"\"\"\n",
    "    def __call__(self, sample: Tuple[np.ndarray, torch.Tensor]) -> Tuple[np.ndarray, torch.Tensor]:\n",
    "        im, domain_vect = sample\n",
    "        if np.random.uniform(low=0.0, high=1.0) > 0.5:\n",
    "            im = np.fliplr(im)\n",
    "        return im, domain_vect\n",
    "    \n",
    "class RandomRotate(object):\n",
    "    \"\"\"Randomly rotates the image.\"\"\"\n",
    "\n",
    "    def __call__(self, sample: Tuple[np.ndarray, torch.Tensor]) -> Tuple[np.ndarray, torch.Tensor]:\n",
    "        rotation_angle_degs = float(np.random.randint(4) * 90.0)\n",
    "        im, domain_vect = sample\n",
    "        im = rotate(im.astype(np.float32), rotation_angle_degs)\n",
    "        return im, domain_vect\n",
    "\n",
    "class RGBToTransmittance(object):\n",
    "    \"\"\"Converts an RGB image to a transmittance image.\"\"\"\n",
    "    def __init__(self, min_transmittance: float = 1e-4, clip_max_transmittance_to_one: bool = True):\n",
    "        self._min_transmittance: float = min_transmittance\n",
    "        self._max_transmittance = 1.0 if clip_max_transmittance_to_one else None\n",
    "        \n",
    "    def __call__(self, sample: Tuple[np.ndarray, torch.Tensor]) -> Tuple[np.ndarray, torch.Tensor]:\n",
    "        rgb_im, domain_vect = sample\n",
    "        trans_im = rgb_im/np.percentile(rgb_im, q=99.0, axis=(0,1))\n",
    "        trans_im = np.maximum(trans_im, self._min_transmittance)\n",
    "        trans_im = np.minimum(trans_im, self._max_transmittance)\n",
    "        return trans_im, domain_vect\n",
    "    \n",
    "class TransmittanceToRGB(object):\n",
    "    \"\"\"Converts the transmittance RGB assuming the 0 transmittance is 1.0 in RGB.\"\"\"\n",
    "    def __call__(self, sample: Tuple[np.ndarray, torch.Tensor]) -> Tuple[np.ndarray, torch.Tensor]:\n",
    "        trans_im, domain_vect = sample\n",
    "        return 10**(-trans_im), domain_vect\n",
    "        \n",
    "class TransmittanceToAbsorbance(object):\n",
    "    \"\"\"Converts a transmittance image to the absorbance image.\n",
    "    \n",
    "    Args:\n",
    "        min_transmittance (optional): The minimum image transmittance. Defaults to 1e-4.\n",
    "        clip_max_transmittance_to_one (optional): If True, the maximum transmittance will be clipped at 1.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, sample: Tuple[np.ndarray, torch.Tensor]) -> Tuple[np.ndarray, torch.Tensor]:\n",
    "        transmittance, domain_vect = sample\n",
    "        return -np.log10(transmittance), domain_vect\n",
    "        \n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Converts to tensor, changes the order from (H, W, C) to (C, H, W)\"\"\"\n",
    "    def __call__(self, sample: Tuple[np.ndarray, np.ndarray]) -> Dict[str, torch.Tensor]:\n",
    "        im, domain_vect = sample\n",
    "        return torch.tensor(im.astype(np.float32).copy().transpose(2, 0, 1)), domain_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "from tempfile import TemporaryDirectory\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiDomainDataset(Dataset):\n",
    "    \"\"\"A custom dataset that contains images from different domains.\n",
    "    \n",
    "    Each time, this dataset returns a random image from each domain.\n",
    "    Args:\n",
    "        multi_domain_samples: A list of samples from different domains. One item in this list contains 1 sample from each domain. The zero-th index\n",
    "            list is for the TMA dataset.\n",
    "        transforms: A list of transforms to be applied on the sample. \n",
    "        input_patch_size_pixels (optional): The size of the input patch in pixels. This is the patch size. Defaults to INPUT_IMAGE_SIZE_PIXELS.\n",
    "        domain_indices (optional): A list that specifies which domains to sample from. Default to None, in which case, samples will be obtained from all domains.\n",
    "    \"\"\"\n",
    "    TMA_MPP = 0.2522 \n",
    "    _HDF5_CHUNK_SIZE = 256\n",
    "    _TMA_DOMAIN_INDEX = 0\n",
    "    def __init__(self, \n",
    "                 multi_domain_samples: List[List[Samples]], \n",
    "                 transforms: List[object],\n",
    "                 input_patch_size_pixels: int = INPUT_IMAGE_SIZE_PIXELS,\n",
    "                 domain_indices: Optional[List[int]] = None,\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        self._transforms = Compose(transforms)\n",
    "        self._multi_domain_samples = multi_domain_samples\n",
    "        self._h5_folder = TemporaryDirectory()\n",
    "        self._input_patch_size_pixels = input_patch_size_pixels\n",
    "        self._num_domains = len(multi_domain_samples)\n",
    "        self._domain_indices = domain_indices\n",
    "        if self._domain_indices is None:\n",
    "            self._domain_indices = list(range(len(multi_domain_samples)))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(x) for x in self._multi_domain_samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns: a tuple of torch.Tensor and the one-hot domain encoded vector.\"\"\"\n",
    "        domain_idx = np.random.choice(self._domain_indices)\n",
    "        return self.get_item_with_domain_idx(idx = idx, domain_idx = domain_idx)\n",
    "    \n",
    "    def get_item_with_domain_idx(self, idx: int, domain_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        all_samples_in_selected_domain = self._multi_domain_samples[domain_idx]\n",
    "        sample = all_samples_in_selected_domain[idx]\n",
    "        im = self._get_slide_platform_patch(sample)\n",
    "        \n",
    "        domain_vector = np.zeros((self._num_domains,))\n",
    "        domain_vector[domain_idx] = 1.0\n",
    "        return self._transforms((im, torch.FloatTensor(domain_vector)))\n",
    "         \n",
    "    def _get_slide_platform_patch(self, sample: Samples) -> np.ndarray:\n",
    "        \"\"\"Gets a patch from the slide platform.\"\"\"\n",
    "        h5_file_name_for_slide_roi = self._h5_file_name_from_sample(sample)\n",
    "        if not os.path.exists(h5_file_name_for_slide_roi):\n",
    "            slide_id = sample.slide_id\n",
    "            row_idx, col_idx = sample.row_idx, sample.col_idx\n",
    "            slide_reference = SlideReference(int(slide_id))\n",
    "            with slide_reference.read_object() as slide, h5py.File(h5_file_name_for_slide_roi, \"w\") as file:\n",
    "                slide_num_rows, slide_num_cols, _ = slide.shape\n",
    "                im = slide.view_at_mpp(slide.mpp, mpp_tolerance=0.05)\n",
    "\n",
    "                # Make sure that we cover the same distance.\n",
    "                row_slice = self._get_valid_slice_within_range(slide_num_rows, row_idx, self._input_patch_size_pixels)\n",
    "                col_slice = self._get_valid_slice_within_range(slide_num_cols, col_idx, self._input_patch_size_pixels)\n",
    "                patch = cv2.resize(im[row_slice, col_slice, :], (self._input_patch_size_pixels, self._input_patch_size_pixels)) / 255.0\n",
    "                he_dataset = file.create_dataset(\n",
    "                    name='he_roi',\n",
    "                    shape=patch.shape,\n",
    "                    dtype=patch.dtype)\n",
    "                he_dataset[:,:,:] = patch\n",
    "        else:\n",
    "            with h5py.File(h5_file_name_for_slide_roi, \"r\") as file:\n",
    "                patch = file['he_roi'][:,:,:]\n",
    "        return patch\n",
    "            \n",
    "    def _h5_file_name_from_sample(self, sample: Samples) -> str:\n",
    "        \"\"\"Generates an h5 file name from the sample.\"\"\"\n",
    "        return os.path.join(self._h5_folder.name, f\"slide_{sample.slide_id}_roi_row_{sample.row_idx}_col_{sample.col_idx}.h5\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_valid_slice_within_range(max_dimension_pixels: int, center_idx: int, slice_length: int) -> slice:\n",
    "        first_idx = np.clip(center_idx - slice_length // 2, 0, max_dimension_pixels - slice_length)\n",
    "        return slice(first_idx, first_idx + slice_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10326164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the data loader.\n",
    "dataset = MultiDomainDataset(\n",
    "                multi_domain_samples=train_lists,\n",
    "                transforms = [RandomFlip(), RandomRotate(), RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],\n",
    "                domain_indices = [0],\n",
    "            )\n",
    "print(f\"Dataset len = {len(dataset)}\")\n",
    "im, domain_vect = dataset[4]\n",
    "print(f\"Image size = {im.shape}\")\n",
    "print(f\"Domain one-hot = {domain_vect}\")\n",
    "im = np.transpose(im.numpy().astype(np.float),(1,2,0))\n",
    "plt.imshow(im)\n",
    "plt.grid(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3393361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the data loader.\n",
    "dataset = MultiDomainDataset(\n",
    "                multi_domain_samples=val_lists,\n",
    "                transforms = [RandomFlip(), RandomRotate(), RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],\n",
    "                domain_indices = [1],\n",
    "            )\n",
    "print(f\"Dataset len = {len(dataset)}\")\n",
    "im, domain_vect = dataset[40]\n",
    "print(f\"Image size = {im.shape}\")\n",
    "print(f\"Domain one-hot = {domain_vect}\")\n",
    "im = np.transpose(im.numpy().astype(np.float),(1,2,0))\n",
    "plt.imshow(im)\n",
    "plt.grid(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25cb00",
   "metadata": {},
   "source": [
    "## 3. Initializer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63590b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "import torch.nn as nn\n",
    "class InitilizationType(Enum):\n",
    "    KAIMING = auto()\n",
    "    XAVIER = auto()\n",
    "    NORMAL = auto()\n",
    "    \n",
    "class Initializer:\n",
    "    \"\"\"A class that initializes the model weights.\n",
    "    \n",
    "    Args:\n",
    "        init_type (optional): The type of the initialization. Defaults to InitilizationType.NORMAL.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, init_type: InitilizationType = InitilizationType.NORMAL, init_gain: float = 0.02) -> None:\n",
    "        self._init_type = init_type\n",
    "        self._init_gain = init_gain\n",
    "        \n",
    "    def __call__(self, m: nn.Module):\n",
    "        m.apply(self._initialize_module)\n",
    "        return m\n",
    "    \n",
    "    def _initialize_module(self, m: object) -> None:\n",
    "        class_name = m.__class__.__name__\n",
    "        if self._has_weights(m):\n",
    "            if self._is_conv_layer(class_name) or self._is_linear_layer(m):\n",
    "                if self._init_type == InitilizationType.KAIMING:\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n",
    "                elif self._init_type == InitilizationType.XAVIER:\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=self._init_gain)\n",
    "                elif self._init_type == InitilizationType.NORMAL:\n",
    "                    nn.init.normal_(m.weight.data, mean=0, std=self._init_gain)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown initialization type!\")\n",
    "                \n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, val=0)\n",
    "            if self._is_batchnorm2d_layer(class_name):\n",
    "                #TODO: investigate why the mean of this is set to 1.0\n",
    "                nn.init.normal_(m.weight.data, mean = 1.0, std = self._init_gain)\n",
    "                nn.init.constant_(m.bias.data, val = 0)\n",
    "                \n",
    "    @staticmethod\n",
    "    def _has_weights(m: object) -> bool:\n",
    "        return hasattr(m, 'weights')\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_conv_layer(cls_name: str) -> bool:\n",
    "        return cls_name.find('Conv') != -1\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_linear_layer(cls_name: str) -> bool:\n",
    "        return cls_name.find('Linear') != -1\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_batchnorm2d_layer(cls_name: str) -> bool:\n",
    "        return cls_name.find('BatchNorm2d') != -1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c0692d",
   "metadata": {},
   "source": [
    "## 4. Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf016e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils import spectral_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d318da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLUConv2d(nn.Module):\n",
    "    \"\"\"A class that defines a Conv2d follows by Leaky ReLU.\n",
    "    \n",
    "        [Conv2d with reflection pad with optional spectral normalization] -> [LeakyReLU].\n",
    "        \n",
    "    Args:\n",
    "        in_channels: The number of input channels.\n",
    "        out_channels: The number of output channels.\n",
    "        kernel_size: The size of the 2D convolutional kernel.\n",
    "        stride: The stride of the 2D convolutional kernel.\n",
    "        padding: The size of the padding.\n",
    "        enable_spectral_normalization (optional): If True, the spectral normalization will be perform. Defaults to False.\n",
    "            See https://arxiv.org/pdf/1802.05957.pdf for relating details.\n",
    "        enable_instance_norm (optional): If True, instance normalization will be used. Defaults to False.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 kernel_size: int, \n",
    "                 stride: int, \n",
    "                 padding: int, \n",
    "                 enable_spectral_normalization: bool = False,\n",
    "                 enable_instance_norm: bool = False,\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        conv2d_layer = nn.Conv2d(in_channels=in_channels, \n",
    "                                 out_channels=out_channels, \n",
    "                                 kernel_size=kernel_size, \n",
    "                                 stride=stride, \n",
    "                                 padding=padding, \n",
    "                                 padding_mode='reflect',\n",
    "                                 bias=True)\n",
    "        \n",
    "        if enable_spectral_normalization:\n",
    "            conv2d_layer = spectral_norm(conv2d_layer)\n",
    "        \n",
    "        layers = [conv2d_layer,]\n",
    "        \n",
    "        if enable_instance_norm:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels, affine=False))\n",
    "        layers.append(nn.LeakyReLU(inplace=True))\n",
    "            \n",
    "        self._model = nn.Sequential(*layers)\n",
    "        self._model = Initializer(init_type=InitilizationType.NORMAL)(self._model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._model(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3aec4e06",
   "metadata": {},
   "source": [
    "# Test LeakyReLUConv2d only.\n",
    "m = LeakyReLUConv2d(in_channels=3, out_channels=4, kernel_size=7, padding=3, stride=1, enable_spectral_normalization=False, enable_instance_norm=False)\n",
    "x = torch.randn(1, 3, 12, 16)\n",
    "y = m(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUInstNorm2dConv2d(nn.Module):\n",
    "    \"\"\"A class that defines the following transformation.\n",
    "    \n",
    "        [Conv2d with reflection padding] -> [InstanceNorm2d] -> [ReLU]\n",
    "        \n",
    "    Args;\n",
    "        in_channels: The number of input channels.\n",
    "        out_channels: The number of output channels.\n",
    "        kernel_size: The size of the 2D convolutional kernel.\n",
    "        stride: The stride of the 2D convolutional kernel.\n",
    "        padding: The size of the padding.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int) -> None:\n",
    "        super().__init__()\n",
    "        self._model = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                padding_mode='reflect',\n",
    "                bias=True,\n",
    "            ),\n",
    "            \n",
    "            nn.InstanceNorm2d(\n",
    "                num_features=out_channels,\n",
    "                affine=False,\n",
    "            ),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "        self._model = Initializer(init_type=InitilizationType.NORMAL)(self._model)\n",
    "       \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._model(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cbcdcd6",
   "metadata": {},
   "source": [
    "# Test ReLUInstNorm2dConv2d.\n",
    "m = ReLUInstNorm2dConv2d(in_channels=3, out_channels=5, kernel_size=7, stride=1, padding=3)\n",
    "x = torch.randn(1, 3, 12, 16)\n",
    "y = m(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResInstNorm2dConv2d(nn.Module):\n",
    "    \"\"\"A class that defines the residulal condition block that has the following architecture.\n",
    "\n",
    "        - -> [Conv2d(3x3) -> [InstanceNorm2d] -> [ReLU()] -> [Conv2d(3x3)] -> [InstanceNorm2d] -> + ->\n",
    "         |                                                                                        ^\n",
    "         |----------------------------------------------------------------------------------------|\n",
    "         \n",
    "     The numberd of input and output channels are the same.\n",
    "     \n",
    "     Args;\n",
    "        in_channels: The number of input channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self._model = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=in_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                padding_mode='reflect',\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(in_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=in_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                padding_mode='reflect',\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(in_channels),\n",
    "        )\n",
    "        self._model = Initializer(init_type=InitilizationType.NORMAL)(self._model)\n",
    "        \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self._model(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a1d2196",
   "metadata": {},
   "source": [
    "# Test ResInstNorm2dConv2d.\n",
    "m = ResInstNorm2dConv2d(in_channels=3)\n",
    "x = torch.randn(1, 3, 12, 16)\n",
    "y = m(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd37238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentEncoder(nn.Module):\n",
    "    \"\"\"A class that defines the architecture for the content encoder.\n",
    "    \n",
    "    This architecture of this model is:\n",
    "        [LeakyReLUConv2d] -> 3 x [ReLUInstanceNormConv2d] -> 3 x [ResInstNorm2dConv2d]\n",
    "    Args:\n",
    "        in_channels: The number of input channels.\n",
    "        out_channels (optional): The number of output channels. Defaults to NUM_STAIN_VECTORS.\n",
    "        \n",
    "    Returns:\n",
    "        For each minibatch with N samples, it returns a tensor of size N x out_channels x H x W in which \n",
    "        H and W is a downsampled version of the input image.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, num_stain_vectors: int=NUM_STAIN_VECTORS) -> None:\n",
    "        super().__init__()\n",
    "        self._model = nn.Sequential(\n",
    "            LeakyReLUConv2d(in_channels=in_channels, out_channels = 64, kernel_size = 7, stride = 1, padding = 3),\n",
    "            ReLUInstNorm2dConv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 2, padding = 1),\n",
    "            ReLUInstNorm2dConv2d(in_channels = 128, out_channels = num_stain_vectors, kernel_size = 3, stride = 2, padding = 1),\n",
    "            ResInstNorm2dConv2d(in_channels = num_stain_vectors),\n",
    "            ResInstNorm2dConv2d(in_channels = num_stain_vectors),\n",
    "            ResInstNorm2dConv2d(in_channels = num_stain_vectors),\n",
    "            ResInstNorm2dConv2d(in_channels = num_stain_vectors),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539eaa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ContentEncoder.\n",
    "m = ContentEncoder(in_channels=3, num_stain_vectors = NUM_STAIN_VECTORS)\n",
    "x = torch.randn(1, 3, INPUT_IMAGE_SIZE_PIXELS, INPUT_IMAGE_SIZE_PIXELS)\n",
    "y = m(x)\n",
    "print(f\"Input shape = {x.shape}, output shape =  {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5adbada",
   "metadata": {},
   "source": [
    "### Attribute encoder.\n",
    "This network aims to extract the stain vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Downsampling2xkWithSkipConnection(nn.Module):\n",
    "    \"\"\"A class that is a basic block to build the convolutional layers to estimate the parameters of the conditional attribute distribution.\n",
    "    \n",
    "    This block implements the following architecture\n",
    "    x -> -> LeakyReLU -> Conv2d -> LeakyReLU -> Conv2d -> AvgPool2d -> + -> output\n",
    "        |                                                              ^\n",
    "        |-------------> AvgPool2d -> Conv2d ---------------------------|\n",
    "    Args:\n",
    "        in_channels: The number of input channels.\n",
    "        out_channels: The number of output channels.\n",
    "    Reference:\n",
    "        https://github.com/HsinYingLee/MDMM/blob/18360fe3fa37dde28c70c5a945ec783e44eb72ed/networks.py#L334\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super().__init__()\n",
    "        self._forward_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1, padding_mode='reflect', bias=True),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, padding_mode='reflect', bias=True),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self._skip_block = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._forward_block(x) + self._skip_block(x)\n",
    "\n",
    "\n",
    "class StainVectorEstimator(nn.Module):\n",
    "    \"\"\"A class that estimate the stain matrix.\n",
    "    \n",
    "    The networks takes an input image returns a stain vector matrix of size N x (3k^2) x num_stain_vectors.\n",
    "    To generate an output absorbance image, we need to resphae the stain vector into size\n",
    "    (3k^2) * num_stain_vectors and multiply it to a content matrix of num_stain_vectors x H x W to obtain an output image of size\n",
    "    3k^2 * H x W. Then, using the pixel shuffling, we can reduce it to 3 x (kW) x (kH)\n",
    "    \n",
    "    Args:\n",
    "        in_channels: The number of input channels.\n",
    "        out_channels (optional): The number of output channels for the attribute vector. Defaults to NUM_ATTRIBUTES.\n",
    "        downsampling_factor (optional): A factor that describe how much of the image is downsampled. Defaults to 4.\n",
    "    Reference:\n",
    "        https://github.com/HsinYingLee/MDMM/blob/master/networks.py#L64\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, num_stain_vectors: int = NUM_STAIN_VECTORS, downsampling_factor: int = 4) -> None:\n",
    "        super().__init__()\n",
    "        self._num_stain_vectors = num_stain_vectors\n",
    "        self._three_times_k_sqr = (3 * downsampling_factor**2)\n",
    "        self._model = Initializer()(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=4, stride=2, padding=1, padding_mode='reflect', bias=True),\n",
    "                Downsampling2xkWithSkipConnection(in_channels=64, out_channels=128),\n",
    "                Downsampling2xkWithSkipConnection(in_channels=128, out_channels=256),\n",
    "                Downsampling2xkWithSkipConnection(in_channels=256, out_channels=512),\n",
    "                Downsampling2xkWithSkipConnection(in_channels=512, out_channels=1024),\n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1)), # Condense all the X, Y dimensions to 1 pixels. \n",
    "                nn.Conv2d(\n",
    "                    in_channels=1024,\n",
    "                    out_channels=self._num_stain_vectors * self._three_times_k_sqr,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    bias=True),\n",
    "                nn.LeakyReLU(inplace=False),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def forward(self, ims: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns the mean and log of the variance for the conditional distribution z^a|ims.\"\"\"\n",
    "        x1 = self._model(ims)\n",
    "        x2 = x1.view(x1.size(0), self._three_times_k_sqr, self._num_stain_vectors)\n",
    "        # Prevent the attribute from collapsing by normaling it withrespect to the band dimension\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbc6c3df",
   "metadata": {},
   "source": [
    "# Test ContentEncoder.\n",
    "m = StainVectorEstimator(in_channels=3, num_stain_vectors=NUM_STAIN_VECTORS)\n",
    "x = torch.randn(4, 3, INPUT_IMAGE_SIZE_PIXELS, INPUT_IMAGE_SIZE_PIXELS)\n",
    "y = m(x)\n",
    "print(f\"y shape = {y.shape}\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157eabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbsorbanceImGenerator(nn.Module):\n",
    "    \"\"\"A class that computes the product between the content and the attribute.\n",
    "    \n",
    "    This class generates a synthetic image G(z_c, z_a) from the content tensor z_c, attribute tensor z_a.\n",
    "    Args:\n",
    "        out_channels: The number of output channels for the generator.\n",
    "        downsampling_factor (optional): A factor that describe how much of the image is downsampled. Defaults to 2.\n",
    "    \"\"\"\n",
    "    _NUM_FEATURES_PER_FRACTION = 256\n",
    "    def __init__(self, \n",
    "                 downsampling_factor: int = 4,\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        self._shuffle_layer = torch.nn.PixelShuffle(upscale_factor=downsampling_factor)\n",
    "        \n",
    "    def forward(self, z_c: torch.Tensor, z_a: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Generates an image based by the content and the attribute tensor.\n",
    "        \n",
    "        Args:\n",
    "            z_c: The content image tensor, which contains abundance information of the stain. The size should be of size (N, num_stain_vectors, H, W).\n",
    "            z_a: The attribute image tensor. The size should be of size (N, (3k^2), num_stain_vectors)\n",
    "        \n",
    "        Returns:\n",
    "            The generated image tensor of size (N, 3, k*H, k*W)\n",
    "        \"\"\"\n",
    "        if z_c.size(1) != z_a.size(2):\n",
    "            raise ValueError(f\"The number of elements in first dimension of z_c must match the number of elements in the 2nd dimension of z_a\")\n",
    "        num_rows, num_cols = z_c.size(2), z_c.size(3)\n",
    "        x = torch.bmm(z_a, z_c.view(z_c.size(0), z_c.size(1), -1))\n",
    "        x = x.view(x.size(0), x.size(1), num_rows, num_cols)\n",
    "        return self._shuffle_layer (x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f5e5ca6",
   "metadata": {},
   "source": [
    "x = torch.randn(4, 3, INPUT_IMAGE_SIZE_PIXELS, INPUT_IMAGE_SIZE_PIXELS)\n",
    "m1 = StainVectorEstimator(in_channels=3, num_stain_vectors=NUM_STAIN_VECTORS)\n",
    "m2 = ContentEncoder(in_channels=3, num_stain_vectors = NUM_STAIN_VECTORS)\n",
    "m3 = AbsorbanceImGenerator()\n",
    "z_a = m1(x)\n",
    "z_c = m2(x)\n",
    "y = m3(z_c, z_a)\n",
    "print(f\"x.shape = {x.shape}, y.shape = {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealFakeDiscriminator(nn.Module):\n",
    "    \"\"\"A discriminator that aims to classify if an image is real or generated image and predicts the class encoded logits of the input image.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: The number of channels for the input image.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._downsample = nn.AvgPool2d(kernel_size=3, stride=2, padding=1, count_include_pad=False)\n",
    "        self._encoder = self._make_network(in_channels=in_channels)\n",
    "        self._real_fake_predictor = Initializer()(nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=1, stride=1, padding=0, bias=False))\n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_network(in_channels: int) -> nn.Module:\n",
    "        \"\"\"Make a single scale discriminator.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            LeakyReLUConv2d(in_channels=in_channels, out_channels=64, kernel_size=3, padding=1, stride=2, enable_spectral_normalization=True, enable_instance_norm=False),\n",
    "            LeakyReLUConv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=2, enable_spectral_normalization=True, enable_instance_norm=False),\n",
    "            LeakyReLUConv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=2, enable_spectral_normalization=True, enable_instance_norm=False),\n",
    "            LeakyReLUConv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=2, enable_spectral_normalization=True, enable_instance_norm=False),\n",
    "            LeakyReLUConv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1, stride=2, enable_spectral_normalization=True, enable_instance_norm=False),\n",
    "            LeakyReLUConv2d(in_channels=1024, out_channels=1024, kernel_size=3, padding=1, stride=2, enable_spectral_normalization=True, enable_instance_norm=False),\n",
    "         )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self._encoder(x)\n",
    "        return self._real_fake_predictor(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29aceb",
   "metadata": {},
   "source": [
    "## 5. Datamodule definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08942760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a730ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DOMAIN_INDICES = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDomainDataModule(pl.LightningDataModule):\n",
    "    \"\"\"A class that defines the data module for DRIT training and validation.\n",
    "        multi_domain_train_samples: A list of lists that contains the training samples. One inner list for 1 domain.\n",
    "        multi_domain_val_samples: A list of lists that contains the validation samples. One inner list for 1 domain.\n",
    "        batch_size (optional): The number of samples per batch. Defaults to 32.\n",
    "        num_dataloading_workers (optional): The number of worker for data loading. Defaults to 4.\n",
    "        use_pin_memory (optional): If True, pinned memory will be used. Defaults to True.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        multi_domain_train_samples: List[List[Samples]],\n",
    "        multi_domain_val_samples: List[List[Samples]],\n",
    "        batch_size: int = 32,\n",
    "        num_dataloading_workers: int = 4,\n",
    "        use_pin_memory: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._multi_domain_train_samples = multi_domain_train_samples\n",
    "        self._multi_domain_val_samples = multi_domain_val_samples\n",
    "        self._batch_size = batch_size\n",
    "        self._num_dataloading_workers = num_dataloading_workers\n",
    "        self._use_pin_memory = use_pin_memory\n",
    "        \n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\":\n",
    "            self._train_dataset = MultiDomainDataset(\n",
    "                multi_domain_samples=self._multi_domain_train_samples, \n",
    "                transforms = [RandomFlip(), RandomRotate(), RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],\n",
    "                domain_indices=_DOMAIN_INDICES,\n",
    "            )\n",
    "            self._val_dataset = MultiDomainDataset(\n",
    "                multi_domain_samples=self._multi_domain_val_samples,\n",
    "                transforms = [RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],\n",
    "                domain_indices=_DOMAIN_INDICES,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"MultiDomainDataModule datamodule is not defined for non-fit stages!\")\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self._train_dataset,\n",
    "            batch_size=self._batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers = self._num_dataloading_workers,\n",
    "            pin_memory=self._use_pin_memory\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self._val_dataset,\n",
    "            batch_size=self._batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers = self._num_dataloading_workers,\n",
    "            pin_memory=self._use_pin_memory\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49f5db",
   "metadata": {},
   "source": [
    "## 6. Training module definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "import pytorch_lightning as pl\n",
    "from pathlib import Path\n",
    "from contextlib import ExitStack\n",
    "from tempfile import TemporaryDirectory\n",
    "from pathai.dynamic import NodeFamilies\n",
    "from pathai.research_dev.modules.module_factory import torch_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eec0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \"\"\"A class that defines the loss for the DRIT.\n",
    "    \n",
    "    Args:\n",
    "        real_fake_weight (optional): The weight for penalizing a real vs. fake image. Defaults to 1.0.\n",
    "        recon_weight (optional): The weight for the reconstruction losses. Defaults to 10.0.\n",
    "        content_consistency_weight (optional): The weight for L2 regularization for content consistency. Defaults to 1.0.\n",
    "        attr_consistency_weight (optional): The weight for the latent regression loss. Defaults to 10.0.\n",
    "        mode_seeking_loss_weight (optional): The weight for the mode seeking loss. Defaults to 1.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 real_fake_weight: float = 1.0,\n",
    "                 recon_weight: float = 1.0,\n",
    "                 content_consistency_weight: float = 1.0,\n",
    "                 attr_consistency_weight: float = 10.0,\n",
    "                 mode_seeking_loss_weight: float = 1.0,\n",
    "                ) -> None:\n",
    "    \n",
    "        self._real_fake_weight: float = real_fake_weight\n",
    "        self._recon_weight: float = recon_weight\n",
    "        self._content_consistency_weight: float = content_consistency_weight\n",
    "        self._attr_consistency_weight: float = attr_consistency_weight\n",
    "        self._mode_seeking_loss_weight: float = mode_seeking_loss_weight\n",
    "        \n",
    "        \n",
    "    def compute_generator_and_encoder_losses(self, \n",
    "                                             forward_outputs: Dict[str, torch.Tensor], \n",
    "                                             real_vs_cross_translation_disc: nn.Module,\n",
    "                                             num_permutations_for_fake_images: int\n",
    "                                            ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Computes a dictionary of the losses, keyed by the loss types.\n",
    "        \n",
    "        Args:\n",
    "            forward_outputs: A dictionary of the output tensors from the forward pass, keyed by the name of the outputs.\n",
    "            real_vs_cross_translation_disc: The discriminator that discrimates between the real and the fake cross-translation images.\n",
    "            num_permutations_for_fake_images: The number of permutation used to generate fake images.\n",
    "        \"\"\"\n",
    "        fake_one_time_ims_pred_logits_with_real_vs_cross_trans_disc = real_vs_cross_translation_disc.forward(forward_outputs['fake_one_time_cross_translation_ims'])\n",
    "        \n",
    "        real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc = self._real_fake_loss_for_images(\n",
    "            predicted_logits=fake_one_time_ims_pred_logits_with_real_vs_cross_trans_disc,\n",
    "            target_disc_label=1  # Ref: https://github.com/HsinYingLee/MDMM/blob/master/model.py#L292.\n",
    "        ) / num_permutations_for_fake_images\n",
    "        \n",
    "        self_recon_loss = self._consistency_loss(\n",
    "            real_ims=forward_outputs['real_ims'],\n",
    "            target_ims=forward_outputs['self_recon_ims'],\n",
    "        )\n",
    "        \n",
    "        cont_consistency_loss = (self._latent_regression_loss(forward_outputs['swapped_zc'], forward_outputs['z_c_from_fake_ims']) + \\\n",
    "            self._latent_regression_loss(forward_outputs['z_cs'], forward_outputs['z_c_from_recon_ims'])) / (num_permutations_for_fake_images + 1)\n",
    "       \n",
    "        attribute_consistency_loss = (\n",
    "            self._latent_regression_loss(forward_outputs['z_as'].repeat(num_permutations_for_fake_images, 1, 1), forward_outputs['z_a_from_fake_ims']) + \\\n",
    "            self._latent_regression_loss(forward_outputs['z_as'], forward_outputs['z_a_from_recon_ims'])\n",
    "        ) / (num_permutations_for_fake_images + 1)\n",
    "        \n",
    "        mode_seeking_loss = self._mode_seeking_regularization_loss(zas=forward_outputs['z_as'], ims=forward_outputs['real_ims'])\n",
    "        \n",
    "        total_loss = self._real_fake_weight * real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc + \\\n",
    "            self._recon_weight * self_recon_loss + \\\n",
    "            self._content_consistency_weight * cont_consistency_loss + \\\n",
    "            self._attr_consistency_weight * attribute_consistency_loss + self._mode_seeking_loss_weight * mode_seeking_loss\n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc': real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc,\n",
    "            'self_recon_loss': self_recon_loss,\n",
    "            'cont_consistency_loss': cont_consistency_loss,\n",
    "            'attribute_consistency_loss': attribute_consistency_loss,\n",
    "            'mode_seeking_loss': mode_seeking_loss,\n",
    "            'batch_size': len(forward_outputs['real_ims']),\n",
    "        }\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _consistency_loss(real_ims: torch.Tensor, target_ims: torch.Tensor) -> float:\n",
    "        \"\"\"Computes the  consistency loss between the real images and the cycle reconstructed images.\n",
    "        \n",
    "        Args:\n",
    "            real_ims: The real images.\n",
    "            target_ims: The target images to compare to.\n",
    "        \n",
    "        References:\n",
    "            https://github.com/HsinYingLee/MDMM/blob/master/model.py#L254\n",
    "        \"\"\"\n",
    "        return nn.L1Loss(reduction='sum')(real_ims, target_ims) / (real_ims.size(1) * real_ims.size(2) * real_ims.size(3))\n",
    "         \n",
    "    @staticmethod\n",
    "    def _latent_regression_loss(z1: torch.Tensor, z2: torch.Tensor) -> float:\n",
    "        \"\"\"Computes the latent regression loss, which is an L1 distance between the known encoding tensor 'known_zas' to the mean of the conditional p(z_a|x).\"\"\"\n",
    "        return nn.L1Loss(reduction='mean')(z1, z2) * z1.size(1)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _mode_seeking_regularization_loss(zas: torch.Tensor, ims: torch.Tensor) -> float:\n",
    "        half_batch_size = zas.size(0) // 2\n",
    "        za_dist = nn.L1Loss(reduction='sum')(zas[:half_batch_size], zas[half_batch_size:]) /  zas.size(1)\n",
    "        im_dist = nn.L1Loss(reduction='sum')(ims[:half_batch_size], ims[half_batch_size:]) / (ims.size(1) * ims.size(2) * ims.size(3))\n",
    "        return im_dist / za_dist\n",
    "    \n",
    "    def compute_real_fake_discriminator_losses(self, \n",
    "                                               forward_outputs: Dict[str, torch.Tensor], \n",
    "                                               real_vs_cross_translation_disc: nn.Module,\n",
    "                                               num_permutations_for_fake_images: int,\n",
    "                                              ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Computes a dictionary of the losses for the real vs. fake discriminators, keyed by the loss types.\n",
    "        \n",
    "        Args:\n",
    "            forward_outputs: A dictionary of the output tensors from the forward pass, keyed by the name of the outputs.\n",
    "            real_vs_cross_translation_disc: The discriminator that discrimates between the real and the fake cross-translation images.\n",
    "            num_permutations_for_fake_images: The number of permutation used to generate fake images.\n",
    "        \"\"\"\n",
    "        # The detach() command below makes sure that we have the tensor available after the backward pass of the discriminator so that we can update the generator.\n",
    "        pred_logits_real_vs_cross_trans_disc = real_vs_cross_translation_disc.forward(\n",
    "            torch.cat([forward_outputs['real_ims'].detach(), forward_outputs['fake_one_time_cross_translation_ims'].detach()], dim = 0)\n",
    "        )\n",
    "        \n",
    "        num_real_samples = forward_outputs['real_ims'].size(0)\n",
    "        \n",
    "        real_fake_loss_real_ims_with_real_vs_cross_trans_disc = self._real_fake_loss_for_images(\n",
    "            predicted_logits=pred_logits_real_vs_cross_trans_disc[:num_real_samples],\n",
    "            target_disc_label=1\n",
    "        )\n",
    "        \n",
    "        real_fake_loss_cross_trans_ims_with_real_vs_cross_trans_disc = 0\n",
    "        for fake_image_batch_idx in range(num_permutations_for_fake_images):\n",
    "            real_fake_loss_cross_trans_ims_with_real_vs_cross_trans_disc += self._real_fake_loss_for_images(\n",
    "                predicted_logits=pred_logits_real_vs_cross_trans_disc[num_real_samples * (fake_image_batch_idx + 1):num_real_samples * (fake_image_batch_idx + 2)],\n",
    "                target_disc_label=0\n",
    "            )\n",
    "        # We need to account for the fake that we have more fake images.\n",
    "        real_fake_loss_cross_trans_ims_with_real_vs_cross_trans_disc = real_fake_loss_cross_trans_ims_with_real_vs_cross_trans_disc / num_permutations_for_fake_images\n",
    "        \n",
    "        total_loss = (real_fake_loss_real_ims_with_real_vs_cross_trans_disc + real_fake_loss_cross_trans_ims_with_real_vs_cross_trans_disc)\n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'real_fake_loss_real_ims_with_real_vs_cross_trans_disc': real_fake_loss_real_ims_with_real_vs_cross_trans_disc,\n",
    "            'real_fake_loss_cross_trans_ims_with_real_vs_cross_trans_disc': real_fake_loss_cross_trans_ims_with_real_vs_cross_trans_disc,\n",
    "            'batch_size': len(forward_outputs['real_ims']),\n",
    "        }    \n",
    "        \n",
    "            \n",
    "    @staticmethod\n",
    "    def _real_fake_loss_for_images(predicted_logits: torch.Tensor, target_disc_label: int) -> float:\n",
    "        \"\"\"Computes the adversarial loss when the discriminator is trying to discrimate between the reals and the fake images.\n",
    "        \n",
    "        The loss is given as -{Sum_over_real_images [log(sigmoid(disc(real)))] + Sum_over_fake_images [log(1 - sigmoid(disc(real)))]}\n",
    "        The loss is per-pixel averaged and summed over instances in the mini-batch.\n",
    "        \n",
    "        Args:\n",
    "            predicted_logits: The outputs logits predicted from a discriminator that is used to tell if an image is real or fake. \n",
    "            target_disc_label: The target label that we want the output of the discriminator to be. This can be 0 for Fake and 1 for real image.\n",
    "        \"\"\"\n",
    "        if target_disc_label == 0:\n",
    "            target_labels = torch.zeros_like(predicted_logits, device=predicted_logits.device)\n",
    "        else:\n",
    "            target_labels = torch.ones_like(predicted_logits, device=predicted_logits.device)\n",
    "        return nn.BCEWithLogitsLoss(reduction='sum')(predicted_logits, target_labels) / (predicted_logits.size(2) * predicted_logits.size(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import itertools\n",
    "import logging\n",
    "import cv2\n",
    "class MutliClassTrainingModule(pl.LightningModule):\n",
    "    \"\"\"A class that defines the training module for multi-class image translation.\n",
    "    \n",
    "    Args:\n",
    "        num_input_channels: The number of the input channels.\n",
    "        train_hyperparams: A dictionary that defines different hyperparameters for the training.\n",
    "        test_dataset (optional): The dataset that contains images that is used to visualize the performance of the network over epochs of the training. Defaults to None.\n",
    "    \"\"\"\n",
    "    _RANDOM_SEED = 1\n",
    "    _NUM_DISC_UPDATE_PER_ITERATION = 1\n",
    "    _NUM_PERMUTATION_FOR_FAKE_IMAGES = 3\n",
    "    def __init__(self, \n",
    "                 num_input_channels: int,\n",
    "                 train_hyperparams: Dict[str, Any],\n",
    "                 test_dataset: Optional[MultiDomainDataset] = True,\n",
    "                 **kwargs\n",
    "                 ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._num_input_channels: int = num_input_channels\n",
    "        self._train_hyperparams = train_hyperparams\n",
    "        self._number_gen_optimization_steps_to_update_disc = train_hyperparams['number_gen_optimization_steps_to_update_disc']\n",
    "        \n",
    "        if self._number_gen_optimization_steps_to_update_disc > 1:\n",
    "            print(f\"[WARNING] Please make sure that you have at least {self._number_gen_optimization_steps_to_update_disc} minibatches so that the discrimonator are updated\")\n",
    "        self._enc_c = ContentEncoder(in_channels=3, num_stain_vectors = NUM_STAIN_VECTORS)\n",
    "        self._enc_a = StainVectorEstimator(in_channels=3, num_stain_vectors=NUM_STAIN_VECTORS)\n",
    "        self._gen = AbsorbanceImGenerator()\n",
    "        self._encoders_gen_params = itertools.chain([*self._enc_c.parameters(), *self._enc_a.parameters(), *self._gen.parameters()])\n",
    "        self._disc1 = RealFakeDiscriminator(in_channels=num_input_channels)\n",
    "        \n",
    "        if train_hyperparams['pretrained_model_path'] is not None:\n",
    "            self._enc_c = load_trained_model_from_checkpoint(train_hyperparams['pretrained_model_path'], network=self._enc_c, \n",
    "                                                            starts_str = \"_enc_c.\")\n",
    "            self._enc_a = load_trained_model_from_checkpoint(train_hyperparams['pretrained_model_path'], network=self._enc_a, \n",
    "                                                            starts_str = \"_enc_a.\")\n",
    "            self._gen = load_trained_model_from_checkpoint(train_hyperparams['pretrained_model_path'], network=self._gen, \n",
    "                                                            starts_str = \"_gen.\")\n",
    "            self._disc1 = load_trained_model_from_checkpoint(train_hyperparams['pretrained_model_path'], network=self._disc1, \n",
    "                                                            starts_str = \"_disc1.\")\n",
    "        self._discs_params = self._disc1.parameters()\n",
    "        self._loss = Loss(**train_hyperparams['loss_weights_by_name'])\n",
    "        \n",
    "        self._test_dataset = test_dataset\n",
    "        if test_dataset is not None:\n",
    "            self._temp_save_folder: str = TemporaryDirectory()\n",
    "            print(f\"Training results will be saved to {self._temp_save_folder.name}\")\n",
    "            \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int, optimizer_idx: int) -> Dict[str, torch.Tensor]:\n",
    "        outputs = self._compute_network_forward_outputs(batch)\n",
    "        if optimizer_idx == 0:\n",
    "            self._set_requires_gradients([self._enc_c, self._enc_a, self._gen], requires_grad=True)\n",
    "            self._set_requires_gradients([self._disc1], requires_grad=False)\n",
    "            return self._loss.compute_generator_and_encoder_losses(\n",
    "                forward_outputs=outputs,\n",
    "                real_vs_cross_translation_disc=self._disc1,\n",
    "                num_permutations_for_fake_images=MutliClassTrainingModule._NUM_PERMUTATION_FOR_FAKE_IMAGES,\n",
    "            )\n",
    "        \n",
    "        if optimizer_idx == 1:\n",
    "            self._set_requires_gradients([self._enc_c, self._enc_a, self._gen], requires_grad=False)\n",
    "            self._set_requires_gradients([self._disc1], requires_grad=True)\n",
    "            return self._loss.compute_real_fake_discriminator_losses(\n",
    "                forward_outputs=outputs,\n",
    "                real_vs_cross_translation_disc=self._disc1,\n",
    "                num_permutations_for_fake_images=MutliClassTrainingModule._NUM_PERMUTATION_FOR_FAKE_IMAGES,\n",
    "            )\n",
    "            \n",
    "            \n",
    "    def _compute_network_forward_outputs(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        minibatch_size = batch[0].shape[0]\n",
    "        self._ensure_minibatch_size_is_even(minibatch_size)\n",
    "        real_ims, _ = batch\n",
    "        z_cs = self._enc_c(real_ims)\n",
    "        z_as = self._enc_a(real_ims)\n",
    "        \n",
    "        num_permutations = MutliClassTrainingModule._NUM_PERMUTATION_FOR_FAKE_IMAGES\n",
    "        swapped_zc = self._generate_permutations(z_cs, num_permutations=num_permutations)\n",
    "        # First time cross-translation\n",
    "        input_z_cs = torch.cat([z_cs, swapped_zc], dim=0)\n",
    "        input_z_as = torch.cat([z_as] * (num_permutations + 1), dim=0)\n",
    "        \n",
    "        out_fakes = self._gen(input_z_cs, input_z_as)\n",
    "        self_recon_ims, fake_one_time_cross_translation_ims = torch.split(out_fakes, [minibatch_size, minibatch_size * num_permutations], dim=0)\n",
    "        \n",
    "        # Extract the content tensors.\n",
    "        z_c_from_recon_ims, z_c_from_fake_ims = torch.split(self._enc_c(out_fakes), [minibatch_size, minibatch_size * num_permutations], dim=0)\n",
    "        \n",
    "        z_a_from_recon_ims, z_a_from_fake_ims = torch.split(self._enc_a(out_fakes), [minibatch_size, minibatch_size * num_permutations], dim=0)\n",
    "        return {\n",
    "            'real_ims': real_ims,\n",
    "            'z_cs': z_cs,\n",
    "            'swapped_zc': swapped_zc,\n",
    "            'z_c_from_fake_ims': z_c_from_fake_ims,\n",
    "            'z_c_from_recon_ims': z_c_from_recon_ims,\n",
    "            \n",
    "            'self_recon_ims': self_recon_ims,\n",
    "            'fake_one_time_cross_translation_ims': fake_one_time_cross_translation_ims,\n",
    "            \n",
    "            'z_as': z_as,\n",
    "            'z_a_from_recon_ims': z_a_from_recon_ims,\n",
    "            'z_a_from_fake_ims': z_a_from_fake_ims,\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _generate_permutations(minibatch_samples: torch.Tensor, num_permutations: int = 1) -> torch.Tensor:\n",
    "        num_samples = minibatch_samples.size(0)\n",
    "        all_permutations: List[torch.Tensor]=[]\n",
    "        for _ in range(num_permutations):\n",
    "            sample_permutation = torch.randperm(num_samples, device = minibatch_samples.device)\n",
    "            sample_permutation = sample_permutation.view(num_samples, 1, 1, 1)\n",
    "            sample_permutation = sample_permutation.repeat(1, minibatch_samples.size(1), minibatch_samples.size(2), minibatch_samples.size(3))\n",
    "            all_permutations.append(torch.gather(minibatch_samples, 0, sample_permutation))\n",
    "        return torch.cat(all_permutations, dim=0)\n",
    "    \n",
    "   \n",
    "\n",
    "    @staticmethod\n",
    "    def _sample_attribute_vectors_from_gaussian_distribution(mus: torch.Tensor, logvars: torch.Tensor, over_sampling_factor: int = 1) -> torch.Tensor:\n",
    "        std = logvars.mul(0.5).exp()\n",
    "        z = MutliClassTrainingModule._sample_normal_distrbution(num_samples=mus.size(0) * over_sampling_factor, attr_dim=mus.size(1), target_device=mus.device)\n",
    "        z = torch.randn(mus.shape, device=mus.device)\n",
    "        z = z.mul(std).add(mus)\n",
    "        return z.view(z.size(0), z.size(1), 1, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _aggregate_losses_from_generator_and_discrimonator_loss_dicts(gen_loss_dict: Dict[str, torch.Tensor], disc_loss_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Returns a new dictionary of loss terms which is the combination of losses from two loss dictionaries.\"\"\"\n",
    "        losses_by_name: Dict[str, torch.Tensor] = {\n",
    "            'gen_loss': gen_loss_dict['loss'],\n",
    "            'disc_loss': disc_loss_dict['loss'],\n",
    "        }\n",
    "        \n",
    "        for loss_dict in [gen_loss_dict, disc_loss_dict]:\n",
    "            for k, v in loss_dict.items():\n",
    "                if k != 'loss':\n",
    "                    losses_by_name[k] = v\n",
    "        if set(gen_loss_dict.keys()).intersection(disc_loss_dict.keys()) != {\"loss\", \"batch_size\"}:\n",
    "            raise ValueError(\"Generator loss dictionary and discriminator loss dictionary have overlapping names!\")\n",
    "        return losses_by_name\n",
    "                \n",
    "        \n",
    "    def training_step_end(self, workers_outputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Aggregates the results from the training steps across multiple workers in the same batch.\n",
    "\n",
    "        This is required for gradient descent to work. Otherwise, we would get \"RuntimeError: grad can be implicitly\n",
    "        created only for scalar outputs\" because the loss is not a scalar (i.e., tensor with one element); instead it\n",
    "        would be a tensor with size equal to the number of workers.\n",
    "        \n",
    "        Args:\n",
    "            workers_outputs: A dictionary that contains the outputs from multiple workers.\n",
    "        \"\"\"\n",
    "        return {k: v.sum() for k, v in workers_outputs.items()}\n",
    "            \n",
    "          \n",
    "    def training_epoch_end(self, batch_outputs: List[List[Dict[str,float]]]) -> None:\n",
    "        \"\"\"Combines the loss from all batches.\n",
    "        \n",
    "        Args:\n",
    "            batch_outputs: A list of list in which one item is for one optimizer idx. Each item in the inner\n",
    "                lists is a Dictionary with key equals to 'loss'.\n",
    "        \"\"\"\n",
    "        self._set_random_seed()\n",
    "        total_num_samples = torch.stack([b['batch_size'] for b in batch_outputs[0]]).sum()\n",
    "        avg_generator_and_encoder_loss = torch.stack([b['loss'] for b in batch_outputs[0]]).sum() / total_num_samples    \n",
    "        avg_real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc = torch.stack([b['real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc'] for b in batch_outputs[0]]).sum() / total_num_samples\n",
    "        avg_self_recon_loss = torch.stack([b['self_recon_loss'] for b in batch_outputs[0]]).sum() / total_num_samples\n",
    "        avg_cont_consistency_loss = torch.stack([b['cont_consistency_loss'] for b in batch_outputs[0]]).sum() / total_num_samples\n",
    "        avg_attribute_consistency_loss= torch.stack([b['attribute_consistency_loss'] for b in batch_outputs[0]]).sum() / total_num_samples       \n",
    "        avg_mode_seeking_loss = torch.stack([b['mode_seeking_loss'] for b in batch_outputs[0]]).sum() / total_num_samples\n",
    "        avg_train_disc_total_loss = torch.stack([b['loss'] for b in batch_outputs[1]]).sum() / total_num_samples\n",
    "        avg_real_fake_loss_real_ims_with_real_vs_cross_trans_disc= torch.stack([b['real_fake_loss_real_ims_with_real_vs_cross_trans_disc'] for b in batch_outputs[1]]).sum() / total_num_samples\n",
    "        avg_real_fake_loss_cross_trans_ims_with_real_vs_cross_trans_disc= torch.stack([b['real_fake_loss_cross_trans_ims_with_real_vs_cross_trans_disc'] for b in batch_outputs[1]]).sum() / total_num_samples\n",
    "        \n",
    "        print(f\"\\n-> Epoch {self.current_epoch}: train_encoders_generators_total_loss: {avg_generator_and_encoder_loss:.3f}, \" + \\\n",
    "              f\"\\n        BCE[D_1(cross_tran), 'real'): {avg_real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc:.3f}\" + \"  -> Expected value: -log(0.5) = 0.6931.\" \\\n",
    "              f\"\\n        self_recon_loss: {avg_self_recon_loss:.3f} / cont_consistency_loss: {avg_cont_consistency_loss:.3f} / attribute_consistency_loss: {avg_attribute_consistency_loss:.3f} / mode_seeking_loss: {avg_mode_seeking_loss:.3f}\"\n",
    "             ) \n",
    "        \n",
    "        print(f\"-> train_disc_total_loss: {avg_train_disc_total_loss:.3f}\" + \\\n",
    "              f\"\\n        BCE[D_1(real_image), 'real'): {avg_real_fake_loss_real_ims_with_real_vs_cross_trans_disc:.3f} / BCEL[D_1(cross_trans), 'fake']: {avg_real_fake_loss_cross_trans_ims_with_real_vs_cross_trans_disc:.3f}\" + \" -> Expected value: -log(0.5) = 0.6931.\")\n",
    "            \n",
    "        self.log(\"train_encoders_generators_total_loss\", avg_generator_and_encoder_loss)\n",
    "        self.log(\"train_disc_total_loss\", avg_train_disc_total_loss)\n",
    "        if self._test_dataset is not None and self.current_epoch % 10 == 0:\n",
    "            self._generate_inference_results()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _set_random_seed():\n",
    "        # Make sure that the same set of random tensors are initialized at each epochs to maintain the same dataset for the optimization of the loss function.\n",
    "        torch.manual_seed(MutliClassTrainingModule._RANDOM_SEED)\n",
    "        torch.cuda.manual_seed(MutliClassTrainingModule._RANDOM_SEED)\n",
    "                \n",
    "    def _generate_inference_results(self) -> None:\n",
    "        \"\"\"Generates the inference results for debugging.\"\"\"\n",
    "        tensor_im_0, _ = self._test_dataset.get_item_with_domain_idx(idx=0, domain_idx=0)\n",
    "        tensor_im_1, _ = self._test_dataset.get_item_with_domain_idx(idx=100, domain_idx=0)\n",
    "        im = tensor_im_0.numpy().transpose(1, 2, 0)\n",
    "        gpu_device = torch.device(\"cuda:0\")\n",
    "        tensor_im_0 = tensor_im_0[None, ...].to(gpu_device)\n",
    "        tensor_im_1 = tensor_im_1[None, ...].to(gpu_device)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z_c = self._enc_c.to(gpu_device)(tensor_im_0)\n",
    "            z_a_0 = self._enc_a.to(gpu_device)(tensor_im_0)\n",
    "            z_a_1 = self._enc_a.to(gpu_device)(tensor_im_1)\n",
    "            \n",
    "            recon_im = self._gen.to(gpu_device)(z_c, z_a_0)\n",
    "            cross_recon_im = self._gen.to(gpu_device)(z_c, z_a_1)\n",
    "        \n",
    "        num_total_images = 4\n",
    "        num_rows, num_cols, num_chans = im.shape\n",
    "        out_ims = np.zeros((num_rows, num_total_images * num_cols, num_chans), dtype=np.float)\n",
    "        out_ims[:, :num_cols] = 10**(-im)\n",
    "        out_ims[:, num_cols: 2 * num_cols] = 10**(-recon_im.cpu().numpy()[0].transpose(1,2,0))\n",
    "        out_ims[:, 2 * num_cols: 3 * num_cols] = 10**(-tensor_im_1[0].cpu().numpy().transpose(1, 2, 0))\n",
    "        out_ims[:, 3 * num_cols:] = 10**(-cross_recon_im.cpu().numpy()[0].transpose(1,2,0))\n",
    "        out_ims = (np.clip(out_ims, 0.0, 1.0) * 255.0).astype(np.uint8)\n",
    "        image_name = os.path.join(self._temp_save_folder.name, f\"training_res_epoch_{self.current_epoch}.png\")\n",
    "        cv2.imwrite(image_name, out_ims)\n",
    "        print(f\"Save debug image {image_name}\")\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _set_requires_gradients(networks: List[nn.Module], requires_grad: bool) -> None:\n",
    "        \"\"\"Sets the status for gradient calculation for the networks.\n",
    "        \n",
    "        Args:\n",
    "            networks: A list of neural networks to set the gradient.\n",
    "            requires_grad: The value of required gradient to set. If False, gradient will not be calculated.\n",
    "        \"\"\"\n",
    "        for net in networks:\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        # Optimizer configuration\n",
    "        # See the doc at https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#configure-optimizers\n",
    "        beta_values = (0.5, 0.999)\n",
    "        encoders_generator_opt = torch.optim.SGD(self._encoders_gen_params, lr=train_hyperparams['gen_learning_rate'], weight_decay=0.0001)\n",
    "        discs_opt = torch.optim.SGD(self._discs_params, lr=train_hyperparams['disc_learning_rate'], weight_decay=0.0001)\n",
    "        return {\"optimizer\": encoders_generator_opt, \"lr_scheduler\": {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ExponentialLR(encoders_generator_opt, 0.95,  verbose=True),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": self._train_hyperparams[\"number_of_steps_to_update_lr\"],\n",
    "        }\n",
    "        }, {\"optimizer\": discs_opt, \"lr_scheduler\": {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ExponentialLR(discs_opt, 0.95,  verbose=True),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": self._train_hyperparams[\"number_of_steps_to_update_lr\"],\n",
    "        }}\n",
    "    \n",
    "    def optimizer_step(\n",
    "        self,\n",
    "        epoch,\n",
    "        batch_idx,\n",
    "        optimizer,\n",
    "        optimizer_idx,\n",
    "        optimizer_closure,\n",
    "        on_tpu=False,\n",
    "        using_native_amp=False,\n",
    "        using_lbfgs=False,\n",
    "    ) -> None:\n",
    "        # See https://pytorch-lightning.readthedocs.io/en/latest/common/optimization.html for more information on how to optimize this correctly.\n",
    "        if optimizer_idx == 0:\n",
    "            if (batch_idx + 1) % self._number_gen_optimization_steps_to_update_disc == 0:\n",
    "                optimizer.step(closure=optimizer_closure)\n",
    "            else:\n",
    "                optimizer_closure()\n",
    "        \n",
    "        # Update the discriminator for each iteration\n",
    "        if optimizer_idx == 1:\n",
    "            optimizer.step(closure=optimizer_closure)\n",
    "            \n",
    "\n",
    "    def validation_step(self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> Dict[str, float]:\n",
    "        outputs = self._compute_network_forward_outputs(batch)\n",
    "        return self._loss.compute_generator_and_encoder_losses(\n",
    "                forward_outputs=outputs,\n",
    "                real_vs_cross_translation_disc=self._disc1,\n",
    "                num_permutations_for_fake_images=MutliClassTrainingModule._NUM_PERMUTATION_FOR_FAKE_IMAGES,\n",
    "            )\n",
    "    \n",
    "    @staticmethod\n",
    "    def _ensure_minibatch_size_is_even(minibatch_size: int) -> None:\n",
    "        if not minibatch_size % 2 == 0:\n",
    "            raise ValueError(f\"The size of the minibatch must be even! The requested minibatch size is {minibatch_size}.\")\n",
    "            \n",
    "    @staticmethod\n",
    "    def _sample_normal_distrbution(num_samples: int, attr_dim: int, target_device: torch.device) -> torch.Tensor:\n",
    "        return torch.randn((num_samples, attr_dim, 1, 1), device=target_device)\n",
    "         \n",
    "    def validation_step_end(self, workers_outputs: Dict[str, torch.Tensor]):\n",
    "        \"\"\"Combines the loss from all workers.\"\"\"\n",
    "        return {k: v.sum() for k, v in workers_outputs.items()}\n",
    "        \n",
    "    \n",
    "    def validation_epoch_end(self, batch_outputs: List[Dict[str,float]]):\n",
    "        \"\"\"Combines the loss from all batches.\n",
    "        \n",
    "        Args:\n",
    "            batch_outputs: A list in which each item is a Dictionary that contains information from each batch.\n",
    "        \"\"\"\n",
    "        self._set_random_seed()\n",
    "        total_num_samples = torch.stack([b['batch_size'] for b in batch_outputs]).sum()\n",
    "        val_avg_generator_and_encoder_loss = torch.stack([b['loss'] for b in batch_outputs]).sum() / total_num_samples\n",
    "        \n",
    "        avg_real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc = torch.stack([b['real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc'] for b in batch_outputs]).sum() / total_num_samples\n",
    "        \n",
    "        avg_self_recon_loss = torch.stack([b['self_recon_loss'] for b in batch_outputs]).sum() / total_num_samples\n",
    "        avg_cont_consistency_loss = torch.stack([b['cont_consistency_loss'] for b in batch_outputs]).sum() / total_num_samples\n",
    "        avg_attribute_consistency_loss = torch.stack([b['attribute_consistency_loss'] for b in batch_outputs]).sum() / total_num_samples        \n",
    "        avg_mode_seeking_loss = torch.stack([b['mode_seeking_loss'] for b in batch_outputs]).sum() / total_num_samples\n",
    "        \n",
    "        print(f\"\\n-> Val: train_encoders_generators_total_loss: {val_avg_generator_and_encoder_loss:.3f}, \" + \\\n",
    "             f\"\\n        real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc: {avg_real_fake_adv_loss_cross_trans_ims_with_real_vs_cross_trans_disc:.3f}\" + \\\n",
    "             f\"\\n        self_recon_loss: {avg_self_recon_loss:.3f} / cont_consistency_loss: {avg_cont_consistency_loss:.3f} / attribute_consistency_loss: {avg_attribute_consistency_loss:.3f} / mode_seeking_loss: {avg_mode_seeking_loss:.3f}.\" \n",
    "             ) \n",
    "        self.log(\"val_encoders_generators_total_loss\", val_avg_generator_and_encoder_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS_ON_MLE_ENV = 8\n",
    "def _get_num_gpu_for_pytorch_training() -> int:\n",
    "    return NUM_GPUS_ON_MLE_ENV if torch.cuda.is_available() else 0\n",
    "\n",
    "num_gpus = _get_num_gpu_for_pytorch_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814d693",
   "metadata": {},
   "source": [
    "## 7. Callback definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef428a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model_from_checkpoint(checkpoint_url: str, network: nn.Module, starts_str: str) -> nn.Module:\n",
    "    \"\"\"Loads the model from the checkpoint on s3.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_url: The path to the trained model on S3.\n",
    "        network: A network to load the checkpoint parameters to.\n",
    "        starts_str: A first few letters of the network variable to load the checkpoint from.\n",
    "\n",
    "    Returns:\n",
    "        The loaded model.\n",
    "    \"\"\"\n",
    "    local_model_checkpoint = sync_files_from_s3_url([checkpoint_url])[0]\n",
    "    state_dict = torch.load(local_model_checkpoint)[\"state_dict\"]\n",
    "    state_dict = OrderedDict([k[len(starts_str) :], v] for k, v in state_dict.items() if k.startswith(starts_str))\n",
    "    network.load_state_dict(state_dict)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model_from_checkpoint(checkpoint_url: str, network: nn.Module, starts_str: str) -> nn.Module:\n",
    "    \"\"\"Loads the model from the checkpoint on s3.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_url: The path to the trained model on S3.\n",
    "        network: A network to load the checkpoint parameters to.\n",
    "        starts_str: A first few letters of the network variable to load the checkpoint from.\n",
    "\n",
    "    Returns:\n",
    "        The loaded model.\n",
    "    \"\"\"\n",
    "    local_model_checkpoint = sync_files_from_s3_url([checkpoint_url])[0]\n",
    "    state_dict = torch.load(local_model_checkpoint)[\"state_dict\"]\n",
    "    state_dict = OrderedDict([k[len(starts_str) :], v] for k, v in state_dict.items() if k.startswith(starts_str))\n",
    "    network.load_state_dict(state_dict)\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084a639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pathai.handlers.s3 import sync_folder_to_s3\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pathai.handlers.s3 import sync_files_from_s3\n",
    "from pathai.handlers.s3 import sync_files_from_s3_url\n",
    "\n",
    "def every_n_checkpoint_callback(save_dir: str, every_n_train_epoch: int = 1) -> ModelCheckpoint:\n",
    "    \"\"\"Returns a model checkpoint callback, which saves the model every n epochs.\n",
    "\n",
    "    Args:\n",
    "        save_dir: The path to the saving directory.\n",
    "        every_n_train_epoch (optional): A number that specifies the number of epoch interval that the checkpoint will be\n",
    "            saved. Defaults to 1.\n",
    "    \"\"\"\n",
    "    _make_dir_if_not_exist(save_dir)\n",
    "    return ModelCheckpoint(\n",
    "        dirpath=save_dir,\n",
    "        period=every_n_train_epoch,\n",
    "        filename=\"periodic_{epoch}_{val_encoders_generators_total_loss:.3f}_{train_encoders_generators_total_loss:.3f}_{train_disc_total_loss:.3f}\",\n",
    "        save_top_k=-1,\n",
    "    )\n",
    "\n",
    "def _make_dir_if_not_exist(dir_name: str) -> None:\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "def top_n_checkpoint_callback(save_dir: str, num_best_models: int = 5) -> ModelCheckpoint:\n",
    "    \"\"\"Returns a model checkpoint callback, which saves the top-n performer.\n",
    "\n",
    "    Args:\n",
    "        save_dir: The path to the saving directory.\n",
    "        num_best_models: The number of best performer to save.\n",
    "    \"\"\"\n",
    "    _make_dir_if_not_exist(save_dir)\n",
    "    return ModelCheckpoint(\n",
    "        dirpath=save_dir,\n",
    "        monitor=\"val_encoders_generators_total_loss\",\n",
    "        save_top_k=num_best_models,\n",
    "        mode=\"min\",\n",
    "        filename=\"top_n_{epoch}_{val_encoders_generators_total_loss:.3f}_{train_encoders_generators_total_loss:.3f}_{train_disc_total_loss:.3f}\",\n",
    "    )\n",
    "\n",
    "class SyncEveryNCheckpointsToS3Callback(Callback):\n",
    "    \"\"\"A PyTorch Lightning callback class for syncing a local folder to an S3 directory every n epochs.\n",
    "\n",
    "    This callback is useful when we want to terminate a training process early (e.g., early stopping because model has\n",
    "    converged and/or has taken too long to train) but still want to keep the model checkpoints.\n",
    "\n",
    "    Args:\n",
    "        local_dir: The local directory in which the checkpoints will be saved.\n",
    "        every_n_train_epochs: A number that specifies the epoch interval to save the checkpoints. Defaults to 10.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, local_dir: str, s3_dir: str, every_n_train_epochs: int = 10) -> None:\n",
    "        self._local_dir = local_dir\n",
    "        self._s3_dir = s3_dir\n",
    "        self._every_n_train_epochs = every_n_train_epochs\n",
    "\n",
    "    def on_validation_epoch_end(self, trainner, pl_module: pl.LightningModule) -> None:\n",
    "        if (pl_module.current_epoch + 1) % self._every_n_train_epochs == 0:\n",
    "            sync_folder_to_s3(local_folder=self._local_dir, destination_key=self._s3_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c190d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(node_family=NodeFamilies.GPU, slots=num_gpus)\n",
    "def train_model(num_input_channels: int, \n",
    "                experiment_name: str, \n",
    "                train_hyperparams: Dict[str, Any],\n",
    "                multi_domain_train_samples: List[List[Samples]],\n",
    "                multi_domain_val_samples: List[Tuple[Samples]],\n",
    "               ):\n",
    "    \"\"\"Trains the cycleGAN for the scanner transform.\n",
    "    \n",
    "    Args:\n",
    "        num_input_channels: The number of the input channels.\n",
    "        experiment_name: The name of the experiment to run.\n",
    "        train_hyperparams: The hyperparameters for the training.\n",
    "        multi_domain_train_samples: A list of lists that contains the training samples. One inner list for 1 domain.\n",
    "        multi_domain_val_samples:A list of lists that contains the validation samples. One inner list for 1 domain.\n",
    "    \"\"\"\n",
    "    S3_MODEL_FOLDER = \"tnguyen/breast_cancer/\"\n",
    "    print(\"Training model...\")\n",
    "    with ExitStack() as stack:\n",
    "        from pathai.parameters.config_manager import config_manager\n",
    "        from pathai.research_dev.io.utils import get_tensorboard_logger\n",
    "        \n",
    "        config_manager.set_interactive_mode()\n",
    "        config_manager.load_all_defaults()\n",
    "        if not config_manager.get_task_configuration().env_name:\n",
    "            config_manager.update_task_configuration(dict(env_name = \"tan.nguyen/overlay_test\"))\n",
    "        \n",
    "        tensorboard_logger = get_tensorboard_logger(experiment_name)\n",
    "        print(f\"Tensorboard save dir = {tensorboard_logger.save_dir}, log dir = {tensorboard_logger.log_dir}\")\n",
    "        \n",
    "        with TemporaryDirectory() as save_dir:\n",
    "            check_val_n_epochs = 5\n",
    "            every_n_epochs_dir = os.path.join(save_dir, \"every_n\")\n",
    "            top_n_dir = os.path.join(save_dir, \"top_n\")\n",
    "            every_n_s3_folder_dir = \"imaging-team/\" + str(Path(S3_MODEL_FOLDER) / \"trained_models\" / experiment_name / \"every_n\")\n",
    "            top_n_s3_folder_dir = \"imaging-team/\" + str(Path(S3_MODEL_FOLDER) / \"trained_models\" / experiment_name / \"top_n\")\n",
    "            print(f\"Checkpoint location on s3 for every_n: {every_n_s3_folder_dir}\")\n",
    "            \n",
    "            callbacks=[\n",
    "                every_n_checkpoint_callback(save_dir=every_n_epochs_dir),\n",
    "                top_n_checkpoint_callback(save_dir=top_n_dir),\n",
    "                EarlyStopping(monitor=\"val_encoders_generators_total_loss\", mode=\"min\", min_delta=0.0001, patience=10),\n",
    "                SyncEveryNCheckpointsToS3Callback(local_dir=every_n_epochs_dir,\n",
    "                                                  s3_dir=every_n_s3_folder_dir, \n",
    "                                                  every_n_train_epochs=check_val_n_epochs),\n",
    "            ]\n",
    "            \n",
    "            trainer = pl.Trainer(\n",
    "                callbacks=callbacks,\n",
    "                max_epochs = train_hyperparams['num_epochs'],\n",
    "                progress_bar_refresh_rate = 5,\n",
    "                num_sanity_val_steps=1,\n",
    "                precision=16,\n",
    "                logger = [tensorboard_logger],\n",
    "                resume_from_checkpoint=False,\n",
    "                checkpoint_callback=True,\n",
    "                check_val_every_n_epoch=20,\n",
    "                log_every_n_steps=20,\n",
    "                \n",
    "                #profiler=\"advanced\",\n",
    "                \n",
    "                profiler=None,\n",
    "                \n",
    "                # See: https://pytorch-lightning.readthedocs.io/en/latest/guides/speed.html for 'ddp' vs 'dp'\n",
    "                accelerator=\"dp\",\n",
    "                # -1 to use all the available GPUs.\n",
    "                gpus = -1,\n",
    "                num_nodes = 1,\n",
    "                accumulate_grad_batches=1,\n",
    "                \n",
    "                gradient_clip_val=5,\n",
    "                \n",
    "            )\n",
    "            \n",
    "            data_module = MultiDomainDataModule(\n",
    "                multi_domain_train_samples=multi_domain_train_samples, \n",
    "                multi_domain_val_samples=multi_domain_val_samples,\n",
    "                batch_size=train_hyperparams['batch_size'],\n",
    "                num_dataloading_workers=train_hyperparams['num_dataloaders'],\n",
    "            )\n",
    "            \n",
    "            data_module.setup(\"fit\")\n",
    "            \n",
    "            training_module = torch_factory(MutliClassTrainingModule)(\n",
    "                num_input_channels, \n",
    "                train_hyperparams,\n",
    "                test_dataset = MultiDomainDataset(\n",
    "                    multi_domain_samples=val_lists,\n",
    "                    transforms = [RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],\n",
    "                    input_patch_size_pixels=INPUT_IMAGE_SIZE_PIXELS,\n",
    "                ) if train_hyperparams['periodically_save_training_results'] else None\n",
    "            )\n",
    "                \n",
    "            trainer.fit(training_module, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06989a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _adjust_num_samples_so_that_each_gpu_has_an_even_number_of_samples(org_num_samples: int, num_gpu: int, batch_size_per_gpu: int) -> int:\n",
    "    return (org_num_samples // (num_gpus * batch_size_per_gpu)) * (num_gpus * batch_size_per_gpu)\n",
    "\n",
    "def _adjust_minibatch_size(org_batch_size: int) -> int:\n",
    "    \"\"\"Makes sure that each minibatch size has an even number of samples.\"\"\"\n",
    "    return (org_batch_size // 2) * 2\n",
    "\n",
    "def _loss_string_from_weight_dict(loss_weights_by_name: Dict[str, float]) -> int:\n",
    "    return \"_\".join(f\"{k}_{v}\" for k, v in loss_weights_by_name.items())\n",
    "\n",
    "#pretrain_model_path = \"s3://imaging-team/tnguyen/breast_cancer/trained_models/Stain_separation_19888_bs_22_samples_w_real_fake_weight_1.0_recon_weight_20.0_content_consistency_weight_3.0_attr_consistency_weight_1.0_mode_seeking_loss_weight_1.0_patch_size_512_v1/every_n/periodic_epoch=39_val_encoders_generators_total_loss=1.468_train_encoders_generators_total_loss=1.421_train_disc_total_loss=1.329.ckpt\" \n",
    "pretrain_model_path = None\n",
    "batch_size_per_gpu = _adjust_minibatch_size(org_batch_size=16)  # 28 is the largest number that does not cause OOM.\n",
    "\n",
    "loss_weights_by_name = {\n",
    "    'real_fake_weight': 1.0,\n",
    "    'recon_weight': 20.0,\n",
    "    'content_consistency_weight': 3.0, \n",
    "    'attr_consistency_weight': 1.0,\n",
    "    'mode_seeking_loss_weight': 1.0, \n",
    "}\n",
    "\n",
    "train_params_jabba = {\n",
    "    'weight_decay': 0.001,\n",
    "    'gen_learning_rate': 1e-3,\n",
    "    'disc_learning_rate': 5e-2,\n",
    "    'num_epochs': 10000,\n",
    "    'batch_size': batch_size_per_gpu * NUM_GPUS_ON_MLE_ENV,  \n",
    "    'num_dataloaders': 20,\n",
    "    'loss_weights_by_name': loss_weights_by_name,\n",
    "    'pretrained_model_path': pretrain_model_path,\n",
    "    'number_gen_optimization_steps_to_update_disc': 1,\n",
    "    'number_of_steps_to_update_lr': 1,\n",
    "    'periodically_save_training_results': False,\n",
    "}\n",
    "\n",
    "train_params_local = {\n",
    "    'weight_decay': 0.001,\n",
    "    'gen_learning_rate':1e-3,\n",
    "    'disc_learning_rate': 5e-2,\n",
    "    'num_epochs': 3000,\n",
    "    'batch_size': 16,\n",
    "    'loss_weights_by_name': loss_weights_by_name,\n",
    "    'num_dataloaders': 20, \n",
    "    'pretrained_model_path': pretrain_model_path,\n",
    "    'number_gen_optimization_steps_to_update_disc': 1,\n",
    "    'number_of_steps_to_update_lr': 1,\n",
    "    'periodically_save_training_results': True,\n",
    "}\n",
    "\n",
    "run_on_jabba = True\n",
    "# False: run locally, True: run on jabba\n",
    "if run_on_jabba:\n",
    "    num_train_samples = _adjust_num_samples_so_that_each_gpu_has_an_even_number_of_samples(10000, num_gpus, batch_size_per_gpu)\n",
    "    num_val_samples = _adjust_num_samples_so_that_each_gpu_has_an_even_number_of_samples(5000, num_gpus, batch_size_per_gpu)\n",
    "    train_hyperparams = train_params_jabba\n",
    "else:\n",
    "    num_train_samples = 400\n",
    "    num_val_samples = 16\n",
    "    train_hyperparams = train_params_local\n",
    "\n",
    "experiment_name = f\"Stain_separation_{NUM_STAIN_VECTORS}_stain_vectors_{num_train_samples}_bs_{batch_size_per_gpu}_samples_w_{_loss_string_from_weight_dict(loss_weights_by_name)}_patch_size_512_v2\"\n",
    "print(f\"experiment_name = {experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90969ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_domain_train_samples = [x[:num_train_samples] for x in train_lists]\n",
    "multi_domain_val_samples = [x[:num_val_samples] for x in val_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce9d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_on_jabba:\n",
    "    set_jabba()\n",
    "    train_model.dispatch([\n",
    "        3, \n",
    "        experiment_name, \n",
    "        train_hyperparams,\n",
    "        multi_domain_train_samples,\n",
    "        multi_domain_val_samples,\n",
    "    ], cache_key=experiment_name).wait()\n",
    "else:\n",
    "    set_local()\n",
    "    train_model.dispatch([\n",
    "        3, \n",
    "        experiment_name, \n",
    "        train_hyperparams,\n",
    "        multi_domain_train_samples,\n",
    "        multi_domain_val_samples,\n",
    "    ], cache_key=None).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c9066",
   "metadata": {},
   "source": [
    "## 7. Inference testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 channels\n",
    "NUM_STAIN_VECTORS = 32\n",
    "pretrain_model_path = \"s3://imaging-team/tnguyen/breast_cancer/trained_models/Stain_separation_19888_bs_22_samples_w_real_fake_weight_1.0_recon_weight_20.0_content_consistency_weight_3.0_attr_consistency_weight_1.0_mode_seeking_loss_weight_1.0_patch_size_512_v1/every_n/periodic_epoch=139_val_encoders_generators_total_loss=1.532_train_encoders_generators_total_loss=1.604_train_disc_total_loss=1.224.ckpt\" \n",
    "\n",
    "# 8 channels\n",
    "#NUM_STAIN_VECTORS = 8\n",
    "#pretrain_model_path = \"s3://imaging-team/tnguyen/breast_cancer/trained_models/Stain_separation_8_stain_vectors_19968_bs_16_samples_w_real_fake_weight_1.0_recon_weight_20.0_content_consistency_weight_3.0_attr_consistency_weight_1.0_mode_seeking_loss_weight_1.0_patch_size_512_v1/every_n/periodic_epoch=99_val_encoders_generators_total_loss=3.202_train_encoders_generators_total_loss=2.093_train_disc_total_loss=1.156.ckpt\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0faa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc_c = load_trained_model_from_checkpoint(pretrain_model_path, network=ContentEncoder(in_channels=3, num_stain_vectors = NUM_STAIN_VECTORS), starts_str = \"_enc_c.\").eval()\n",
    "enc_a = load_trained_model_from_checkpoint(pretrain_model_path, network=StainVectorEstimator(in_channels=3, num_stain_vectors=NUM_STAIN_VECTORS), starts_str = \"_enc_a.\").eval()\n",
    "gen = load_trained_model_from_checkpoint(pretrain_model_path, network=AbsorbanceImGenerator(), starts_str = \"_gen.\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a102b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IM_SIZE_PIXELS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiDomainDataset(\n",
    "                    multi_domain_samples=train_lists,\n",
    "                    transforms = [RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],\n",
    "                    input_patch_size_pixels=TEST_IM_SIZE_PIXELS,\n",
    "                )\n",
    "val_dataset = MultiDomainDataset(\n",
    "                    multi_domain_samples=val_lists,\n",
    "                    transforms = [RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],\n",
    "                    input_patch_size_pixels=TEST_IM_SIZE_PIXELS,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be779c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "from pathai.research_dev.utilities.pytorch_utils import move_to_best_device\n",
    "\n",
    "class StainNormalizationInferencer:\n",
    "    \"\"\"A class that performs the inferencing based on the trained model.\n",
    "    \n",
    "    Args:\n",
    "        content_encoder: The trained model of the content encoder.\n",
    "        gen: The trained model of the generator.\n",
    "        max_tile_size_pixels (optional): The maximum title of each size that we ran the inference on, excluding the margin.\n",
    "            The true size that the inferencer will work on will be tile_size_pixels + 2 * tile_margin_pixels.\n",
    "            Defaults to 1024.\n",
    "        tile_margin_pixels (optional): The size of the outer region surrounding the tile in pixels.\n",
    "            We don't use the region of the in boundary region to avoid artifacts. Defaults to 0.\n",
    "        max_polarization_val (optional): All polarization signal larger than this value will be mapped to 255 in the \n",
    "            output. Default to 0.8.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        content_encoder: ContentEncoder,\n",
    "        gen: AbsorbanceImGenerator,\n",
    "        max_tile_size_pixels: int = INPUT_IMAGE_SIZE_PIXELS, \n",
    "        tile_margin_pixels: int = 0,\n",
    "        max_polarization_val: float = 0.8,\n",
    "    ) -> None:\n",
    "        self._content_encoder: ContentEncoder = move_to_best_device(content_encoder)\n",
    "        self._gen: ImageGenerator = move_to_best_device(gen)\n",
    "        self._max_tile_size_pixels: int = max_tile_size_pixels\n",
    "        self._tile_margin_pixels: int = tile_margin_pixels\n",
    "        self._max_polarization_val: float = max_polarization_val\n",
    "        self._tile_size_pixels: int = self._max_tile_size_pixels\n",
    "        self._inner_tile_size_pixels: int = self._tile_size_pixels - 2 * tile_margin_pixels\n",
    "    \n",
    "    def infer_one_image(self, image: Union[np.ndarray, torch.Tensor], \n",
    "                        z_a: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"Runs the inference on a single image.\n",
    "        \n",
    "        Args:\n",
    "            image: A numpy or a Tensor image to run the inference on. This is the absorbance image It must of of dimension (H, W, C). \n",
    "            z_a: The attribute tensor for the domain to reconstruct the image.\n",
    "        Returns:\n",
    "            An 8-bit numpy image that contains the inference result of shape (H, W)\n",
    "        \"\"\"\n",
    "        z_a = move_to_best_device(z_a)\n",
    "        out_image = np.zeros_like(image)\n",
    "        image_shape_2d = image.shape[:2]\n",
    "        for r_slice, c_slice, r_ext_slice, c_ext_slice in self._tile_slices_iterator(image_shape_2d = image_shape_2d):\n",
    "            print(f\"Infering in row slice = {r_slice}, col slice = {c_slice}, r_ext_slice = {r_ext_slice}, c_ext_slice = {c_ext_slice}\")\n",
    "            tile = image[r_ext_slice, c_ext_slice, :]    \n",
    "            tile = np.transpose(tile, (2, 0, 1))  # To (C, H, W)                \n",
    "            if tile.dtype == np.uint8:\n",
    "                tile = tile.astype(np.float32) / 255.0\n",
    "            \n",
    "            tile_output = self._infer_one_tile(tile, z_a = z_a)\n",
    "            out_image[r_slice, c_slice] = tile_output.transpose((1,2,0))\n",
    "        return out_image\n",
    "        \n",
    "    def _tile_slices_iterator(self, image_shape_2d: Tuple[int, int], ) -> Generator[Tuple[slice, slice, slice, slice], None, None]:\n",
    "        \"\"\"Returns a tuple of slices to run the inference and place the infered image.\n",
    "        \n",
    "        Returns:\n",
    "            The row slice to extract the image.\n",
    "            The column slice to extract the image.\n",
    "            The row slice to extract the image.\n",
    "            The column slice to extract the image.\n",
    "        \"\"\"\n",
    "        nrows, ncols = image_shape_2d\n",
    "        for tile_r in range(self._tile_margin_pixels, nrows - self._tile_margin_pixels, self._inner_tile_size_pixels):\n",
    "            for tile_c in range(self._tile_margin_pixels, ncols - self._tile_margin_pixels, self._inner_tile_size_pixels):\n",
    "                \n",
    "                # Adjust the begining of the tile.\n",
    "                if tile_r + self._inner_tile_size_pixels + self._tile_margin_pixels > nrows:\n",
    "                    tile_r = nrows - (self._inner_tile_size_pixels + self._tile_margin_pixels)\n",
    "                if tile_c + self._inner_tile_size_pixels + self._tile_margin_pixels > ncols:\n",
    "                    tile_c = ncols - (self._inner_tile_size_pixels + self._tile_margin_pixels)\n",
    "                \n",
    "                tile_r_start = max(tile_r - self._tile_margin_pixels, 0)\n",
    "                tile_c_start = max(tile_c - self._tile_margin_pixels, 0)\n",
    "                \n",
    "                r_slice, c_slice = slice(tile_r, tile_r + self._inner_tile_size_pixels), slice(tile_c, tile_c + self._inner_tile_size_pixels)\n",
    "                r_ext_slice, c_ext_slice = self._create_extended_slices_to_cover_current_tile(\n",
    "                    (r_slice, c_slice), \n",
    "                    image_shape_2d\n",
    "                )\n",
    "                yield (\n",
    "                    r_slice, \n",
    "                    c_slice,\n",
    "                    r_ext_slice,\n",
    "                    c_ext_slice,\n",
    "                )\n",
    "    \n",
    "    def _create_extended_slices_to_cover_current_tile(self, tile_slices: Tuple[slice, slice], image_shape: Tuple[int, int]):\n",
    "        \"\"\"Returns slices of extended row and columns to cover the current tile.\"\"\"\n",
    "        row_slice, col_slice = tile_slices\n",
    "        nrows, ncols = image_shape\n",
    "        left_limit_to_cover = max(0, col_slice.start - self._tile_margin_pixels)\n",
    "        right_limit_to_cover = min(ncols, col_slice.stop + self._tile_margin_pixels)\n",
    "        \n",
    "        top_limit_to_cover = max(0, row_slice.start - self._tile_margin_pixels)\n",
    "        bottom_limit_to_cover = min(nrows, row_slice.stop + self._tile_margin_pixels)\n",
    "        \n",
    "        return slice(top_limit_to_cover, bottom_limit_to_cover), slice(left_limit_to_cover, right_limit_to_cover)\n",
    "\n",
    "        \n",
    "    def _infer_one_tile(self, tile: np.ndarray, z_a: torch.Tensor) -> np.ndarray:\n",
    "        \"\"\"Performs the inference for a single tile.\n",
    "        \n",
    "        Args:\n",
    "            tile: An input tile of dimensions (C, H, W) with all pixel values in [0.0, 1.0]. The shape should be (1, C, H, W).\n",
    "            z_a: The attribute tensor for the domain to reconstruct the image. The shape should be (1, #attrs, 1, 1).\n",
    "            \n",
    "        Returns:\n",
    "            The inference result of dimension (H, W)\n",
    "        \"\"\"\n",
    "        tile = torch.from_numpy(tile)\n",
    "        tile = tile.float()[None, :, :, :]\n",
    "        tile = move_to_best_device(tile)\n",
    "        with torch.no_grad():\n",
    "            z_c = self._content_encoder(tile)\n",
    "            return torch.squeeze(self._gen(z_c, z_a), axis=0).to(\"cpu\").numpy()  # Single channel image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absorbance_to_transmittance(im: np.ndarray) -> np.ndarray:\n",
    "    im = np.clip(im, 0.0, None)\n",
    "    return 10**(-im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff46502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slide_platform_patch(sample: Samples, input_patch_size_pixels: int) -> np.ndarray:\n",
    "    \"\"\"Gets a patch from the slide platform.\n",
    "    \n",
    "    Args:\n",
    "        sample: A sample object that defines the sampling information of the patch.\n",
    "        input_patch_size_pixels: The size of the patch to be extracted in pixels.\n",
    "    Returns:\n",
    "        An image of size H, W, C.\n",
    "        \n",
    "    \"\"\"\n",
    "    slide_id = sample.slide_id\n",
    "    row_idx, col_idx = sample.row_idx, sample.col_idx\n",
    "    slide_reference = SlideReference(int(slide_id))\n",
    "    with slide_reference.read_object() as slide:\n",
    "        slide_num_rows, slide_num_cols, _ = slide.shape\n",
    "        im = slide.view_at_mpp(slide.mpp, mpp_tolerance=0.05)\n",
    "\n",
    "        # Make sure that we cover the same distance.\n",
    "        row_slice = slice(row_idx - input_patch_size_pixels // 2, row_idx + input_patch_size_pixels // 2)\n",
    "        col_slice = slice(col_idx - input_patch_size_pixels // 2, col_idx + input_patch_size_pixels // 2)\n",
    "        return im[row_slice, col_slice, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0846b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset len = {len(val_dataset)}\")\n",
    "im_0_tensor, domain_vect_0 = val_dataset.get_item_with_domain_idx(idx = 12, domain_idx = 1)\n",
    "im_0 = im_0_tensor.numpy().transpose(1, 2, 0)\n",
    "\n",
    "im_1_tensor, domain_vect_0 = train_dataset.get_item_with_domain_idx(idx =28, domain_idx = 2)\n",
    "im_1 = im_1_tensor.numpy().transpose(1, 2, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the attribute encoder from the 2nd image.\n",
    "z_a0 = enc_a(im_0_tensor[None,:])\n",
    "z_a1 = enc_a(im_1_tensor[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a3010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_translation_inferencer = StainNormalizationInferencer(\n",
    "        content_encoder = enc_c,\n",
    "        gen = gen,\n",
    "        max_tile_size_pixels = TEST_IM_SIZE_PIXELS)\n",
    "\n",
    "self_reconstructed_im = image_translation_inferencer.infer_one_image(\n",
    "    image = im_0, \n",
    "    z_a = z_a0)\n",
    "\n",
    "cross_domain_reconstruction_image = image_translation_inferencer.infer_one_image(\n",
    "    image = im_0, \n",
    "    z_a = z_a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Image size = {im_0.shape}\")\n",
    "print(f\"Domain vector = {domain_vect_0}\")\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(absorbance_to_transmittance(im_0))\n",
    "plt.title(f'Source image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(absorbance_to_transmittance(cross_domain_reconstruction_image))\n",
    "plt.title(f'Translated image')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(absorbance_to_transmittance(im_1))\n",
    "plt.title(f'Target image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c10cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_content(im: torch.Tensor, content_encoder: torch.nn.Module) -> None:\n",
    "    # Visualize different content channels in the images.\n",
    "    zc = content_encoder(move_to_best_device(im))[0].detach().cpu().numpy()\n",
    "    num_chans, im_num_rows, im_num_cols = zc.shape\n",
    "    num_chan_sqrt = np.sqrt(num_chans)\n",
    "    num_row_disp = int(num_chan_sqrt)\n",
    "    num_col_disp = int(np.ceil(num_chans/num_row_disp))\n",
    "    combined_num_rows, combined_num_cols = num_row_disp * im_num_rows, num_col_disp * im_num_cols\n",
    "    combined_im = np.zeros((combined_num_rows, combined_num_cols), dtype=zc.dtype)\n",
    "    for chan_idx, chan in enumerate(zc):\n",
    "        row_idx = chan_idx // num_col_disp\n",
    "        col_idx = chan_idx % num_col_disp\n",
    "        combined_im[row_idx * im_num_rows : (row_idx + 1) * im_num_rows, col_idx * im_num_cols : (col_idx + 1) * im_num_cols] = chan / np.max(chan)\n",
    "    \n",
    "    rgb_im = absorbance_to_transmittance(im[0].numpy().transpose(1,2,0))\n",
    "    #combined_im = cv2.resize(combined_im, (target_num_cols, target_num_rows))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(rgb_im)\n",
    "    plt.axis('off')\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.imshow(combined_im)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94c92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_content(im = im_0_tensor[None,:], content_encoder=enc_c)\n",
    "visualize_content(im = im_1_tensor[None,:], content_encoder=enc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e79067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08723b63",
   "metadata": {},
   "source": [
    "### Illustration for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f5ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset len = {len(val_dataset)}\")\n",
    "im_0_tensor, domain_vect_0 = val_dataset.get_item_with_domain_idx(idx = 158, domain_idx = 1)\n",
    "abs_im_0 = im_0_tensor.numpy().transpose(1, 2, 0)\n",
    "\n",
    "num_target_im = 4\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "    \n",
    "for row_idx in range(num_target_im):\n",
    "    im_1_tensor, domain_vect_0 = val_dataset.get_item_with_domain_idx(idx =row_idx + 10, domain_idx = 0)\n",
    "    z_a1 = enc_a(im_1_tensor[None,:])\n",
    "\n",
    "    cross_domain_reconstruction_image = image_translation_inferencer.infer_one_image(\n",
    "        image = abs_im_0, \n",
    "        z_a = z_a1)\n",
    "\n",
    "    print(f\"Image size = {im_0.shape}\")\n",
    "    print(f\"Domain vector = {domain_vect_0}\")\n",
    "\n",
    "    plt.subplot(3, num_target_im, 1 + row_idx)\n",
    "    plt.imshow(absorbance_to_transmittance(abs_im_0))\n",
    "    plt.axis('off')\n",
    "   \n",
    "    plt.subplot(3, num_target_im, 1 + row_idx + num_target_im)\n",
    "    plt.imshow(absorbance_to_transmittance(cross_domain_reconstruction_image))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(3, num_target_im, 1 + row_idx + 2 * num_target_im)\n",
    "    plt.imshow(absorbance_to_transmittance(im_1_tensor.numpy().transpose(1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9655e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f5b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5029f1d",
   "metadata": {},
   "source": [
    "## Generate visualization of patch RGB data using t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b55028d",
   "metadata": {},
   "source": [
    "### Raw color features before transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa6822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a fair comparison, we will use normalize so that the maximum transmittance is 1.0.\n",
    "vis_dataset_by_domain={\n",
    "    0: MultiDomainDataset(multi_domain_samples=train_lists, transforms = [RGBToTransmittance(), TransmittanceToRGB()],domain_indices=[1]),\n",
    "    1: MultiDomainDataset(multi_domain_samples=train_lists, transforms = [RGBToTransmittance(), TransmittanceToRGB()],domain_indices=[2]),\n",
    "    2: MultiDomainDataset(multi_domain_samples=val_lists, transforms = [RGBToTransmittance(), TransmittanceToRGB()],domain_indices=[0]),\n",
    "    3: MultiDomainDataset(multi_domain_samples=val_lists, transforms = [RGBToTransmittance(), TransmittanceToRGB()],domain_indices=[1]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d97cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from skimage.color import rgb2lab, rgb2hsv, rgb2hed, rgb2gray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422129c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_feature_vector_from_patch_data(im: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Computes a color vector formed by the channel means after convert the input image, which is in RGB, into mulitple colorspaces RGB, HSV, LAB, HED, grayscale.\n",
    "    \n",
    "    Args:\n",
    "        im: An input RGB image. This image is of size (C, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        A color vector of formed by the mean of the channel after transforming into other colorspaces.\n",
    "    \"\"\"\n",
    "    im = im.astype(np.float32)\n",
    "    combined_im = np.concatenate([im, rgb2lab(im), rgb2hsv(im), rgb2hed(im)], axis=0)\n",
    "    return np.mean(combined_im, axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5866e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points_per_domain = 500\n",
    "point_vectors_by_domain = defaultdict(list)\n",
    "for domain, dataset in vis_dataset_by_domain.items():\n",
    "    print(f\"Computing vectors for domain {domain}\")\n",
    "    point_vectors_by_domain[domain] = np.array([color_feature_vector_from_patch_data(dataset[sample_idx][0]) for sample_idx in range(num_points_per_domain)])\n",
    "\n",
    "all_features = np.concatenate(list(point_vectors_by_domain.values()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652304da",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name_by_index = {0: 'p2_gt450', 1: 'p2_ufs', 2: 'p2_at2', 3: 'dp200'}\n",
    "labels = np.array([label_name_by_index[domain] for domain in point_vectors_by_domain.keys() for _ in range(num_points_per_domain)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886cd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4a280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, random_state=123)\n",
    "projected_features = tsne.fit_transform(all_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad4342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df[\"y\"] = labels\n",
    "df[\"feature 1\"] = projected_features[:,0]\n",
    "df[\"feature 2\"] = projected_features[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set(font_scale=1.6)\n",
    "sns.axes_style(\"darkgrid\")\n",
    "sns.set_style(\"white\")\n",
    "sns.scatterplot(x=\"feature 1\", y=\"feature 2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 4),\n",
    "                data=df).set(title=\"Color statistics (t-SNE projection)\") \n",
    "plt.grid(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea7e37",
   "metadata": {},
   "source": [
    "### Perform pairwise translation for each point to.a reference point in the 1st domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca17b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_transform_vis_dataset_by_domain={\n",
    "    0: MultiDomainDataset(multi_domain_samples=train_lists, transforms = [RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],domain_indices=[1]),\n",
    "    1: MultiDomainDataset(multi_domain_samples=train_lists, transforms = [RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],domain_indices=[2]),\n",
    "    2: MultiDomainDataset(multi_domain_samples=val_lists, transforms = [RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],domain_indices=[0]),\n",
    "    3: MultiDomainDataset(multi_domain_samples=val_lists, transforms = [RGBToTransmittance(), TransmittanceToAbsorbance(), ToTensor()],domain_indices=[1]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286c3c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "point_vectors_of_transformed_image_by_domain = defaultdict(list)\n",
    "for domain, dataset in to_transform_vis_dataset_by_domain.items():\n",
    "    print(f\"Computing vectors for domain {domain}\")\n",
    "    if domain == 0:\n",
    "        all_images = []\n",
    "        for idx in range(num_points_per_domain):\n",
    "            im = absorbance_to_transmittance(dataset[idx][0].numpy().transpose(1,2,0))\n",
    "            point_vectors_of_transformed_image_by_domain[domain].append(color_feature_vector_from_patch_data(im))\n",
    "    else:\n",
    "        _TARGET_DOMAIN_IDX = 0\n",
    "        for sample_idx in range(num_points_per_domain):\n",
    "            if sample_idx % 50 == 0:\n",
    "                print(f\"sample_idx = {sample_idx}\")\n",
    "            target_im_tensor = to_transform_vis_dataset_by_domain[_TARGET_DOMAIN_IDX][sample_idx][0]\n",
    "            target_im = absorbance_to_transmittance(target_im_tensor.numpy().transpose(1,2,0))\n",
    "            target_za = enc_a(target_im_tensor[None,:, :, :])\n",
    "            source_absorbance_im = dataset[sample_idx][0].numpy().transpose(1,2,0)\n",
    "            translated_im = image_translation_inferencer.infer_one_image(\n",
    "                image = source_absorbance_im, \n",
    "                z_a = target_za)\n",
    "            translated_im = absorbance_to_transmittance(translated_im)\n",
    "            point_vectors_of_transformed_image_by_domain[domain].append(color_feature_vector_from_patch_data(translated_im))\n",
    "            \n",
    "\n",
    "all_features_after_transformed = np.concatenate(list(point_vectors_of_transformed_image_by_domain.values()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcb3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_features_after_transformed = tsne.fit_transform(all_features_after_transformed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_after_transformed = pd.DataFrame()\n",
    "df_after_transformed[\"y\"] = labels\n",
    "df_after_transformed[\"feature 1\"] = projected_features_after_transformed[:,0]\n",
    "df_after_transformed[\"feature 2\"] = projected_features_after_transformed[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f2196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.set(font_scale=1.6)\n",
    "sns.axes_style(\"darkgrid\")\n",
    "sns.set_style(\"white\")\n",
    "sns.scatterplot(x=\"feature 1\", y=\"feature 2\", hue=df_after_transformed.y.tolist(),\n",
    "                palette=sns.color_palette(\"husl\", 4),\n",
    "                data=df_after_transformed).set(title=\"Color statistics (t-SNE projection)\") \n",
    "plt.grid(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907742cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
